# Técnicas de reducción de la dimensionalidad {#sec-reddim}

En el análisis de datos deportivos es habitual trabajar con conjuntos de datos que incluyen **muchas variables simultáneamente**: métricas físicas (carga externa e interna), indicadores técnicos (pases, tiros, pérdidas), variables contextuales (rival, local/visitante, ritmo de juego), o descriptores derivados de tracking y sensores. Esta riqueza de información es valiosa, pero también introduce dificultades prácticas: variables altamente correlacionadas, redundancia, ruido y una complejidad que puede dificultar tanto el análisis exploratorio como la construcción de modelos fiables. No es raro encontrarse con decenas —o cientos— de variables por jugador, sesión o partido.

En este contexto cobran especial interés las **técnicas de reducción de la dimensionalidad**, cuyo objetivo es representar los datos en un espacio de menor dimensión conservando, en la medida de lo posible, la información relevante. En términos intuitivos, se trata de sustituir un conjunto grande de variables originales por un conjunto más pequeño de **componentes** o **factores latentes** que capturen la estructura principal de variación presente en los datos. Estas técnicas permiten simplificar la representación del rendimiento o del comportamiento deportivo sin tener que ignorar por completo la complejidad del fenómeno.

Las ventajas en deporte son claras. En primer lugar, facilitan la **visualización** y la exploración: por ejemplo, representar perfiles de jugadores o sesiones de entrenamiento en dos o tres dimensiones para detectar agrupamientos, transiciones o valores atípicos. En segundo lugar, ayudan a mitigar problemas comunes en modelización deportiva, como la **multicolinealidad** (por ejemplo, múltiples métricas de carga que reflejan el mismo patrón subyacente) y el riesgo de sobreajuste cuando el número de variables crece respecto al número de observaciones. Además, pueden servir como etapa previa a tareas de ML como clustering, clasificación o detección de anomalías.

Ahora bien, estas técnicas también tienen una limitación relevante en un máster orientado al deporte: la **interpretabilidad**. Los componentes reducidos suelen ser combinaciones de variables originales y, aunque pueden capturar patrones estadísticos de forma eficiente, no siempre se traducen directamente en conceptos deportivos claros (“intensidad”, “fatiga”, “control del juego”, etc.). Por ello, su uso responsable exige combinar el análisis matemático con conocimiento del dominio: interpretar los componentes, validar que tienen sentido deportivo y evitar conclusiones excesivas basadas únicamente en proyecciones en baja dimensión.


Despliega los paneles siguientes para descrubir algunas de las técnicas más conocidas.

::: {.callout-note collapse="true"}
## Análisis de Componentes Principales (PCA)

PCA es una de las técnicas más populares para la reducción de la dimensión. Transforma las variables originales en un nuevo conjunto de variables no correlacionadas llamadas componentes principales. Estos componentes capturan la mayor parte de la variabilidad en los datos y se pueden utilizar para representar los datos en un espacio de menor dimensión.
:::

::: {.callout-note collapse="true"}
## Análisis de factor

Similar a PCA, el análisis de factor busca identificar variables latentes o factores subyacentes que expliquen las relaciones entre las variables originales. Es útil cuando se sospecha que las variables observadas están influenciadas por factores no observados.
:::

::: {.callout-note collapse="true"}
## Reducción de la dimensión basada en selección

Esta técnica implica seleccionar un subconjunto de variables originales en función de algún criterio, como su importancia para el problema o su capacidad de explicar la variabilidad. Algunos métodos de selección incluyen la selección de características y la eliminación de características redundantes.
:::

::: {.callout-note collapse="true"}
## Análisis Discriminante Lineal (LDA)

LDA es una técnica que se utiliza en problemas de clasificación. Busca encontrar una combinación lineal de variables que maximice la separación entre clases en el conjunto de datos, lo que puede reducir la dimensión al tiempo que preserva la información relevante para la clasificación.
:::

::: {.callout-note collapse="true"}
## Descomposición en Valores Singulares (SVD)

SVD es una técnica matricial que se utiliza en la factorización de matrices y la reducción de la dimensión. Es fundamental en métodos como PCA y puede utilizarse para reducir la dimensión de matrices de datos.
:::

::: {.callout-note collapse="true"}
## t-Distributed Stochastic Neighbor Embedding (t-SNE)

t-SNE es una técnica de reducción de la dimensión no lineal que se utiliza comúnmente para visualización de datos. Tiene la capacidad de preservar relaciones locales entre puntos en un espacio de menor dimensión.
:::

::: {.callout-note collapse="true"}
## Autoencoders

Los autoencoders son redes neuronales que se utilizan para aprender representaciones de datos de alta dimensión en un espacio de menor dimensión. Son especialmente útiles en problemas de reducción de la dimensión no lineal.
:::

::: {.callout-note collapse="true"}
## Reducción de la dimensión basada en la ingeniería de características

A veces, la reducción de la dimensión se puede lograr mediante la ingeniería de características (**Feature Engineering**), donde se crean nuevas variables que resumen la información de las originales de manera más efectiva.
:::

En este tema, exploraremos en detalle las técnicas más utilizadas para la reducción de la dimensión, proporcionando ejemplos prácticos y pautas para su aplicación efectiva. Estas técnicas son esenciales para cualquier analista de datos que desee extraer conocimiento valioso de grandes conjuntos de datos y simplificar el proceso de análisis sin perder de vista la interpretación de los resultados.

Por tanto, podemos decir que los métodos de reducción de dimensionalidad son técnicas que se utilizan para reducir la cantidad de variables en un conjunto de datos mientras se intenta retener la información relevante. Sin embargo, existen enfoques adicionales para realizar la reducción de la dimensionalidad, que se clasifican comúnmente en tres categorías: *filter*, *wrapper* y *embedded* [@john1994irrelevant].

::: {.callout-note collapse="true"}
## Métodos filter

Los métodos filter son técnicas de selección de características que se aplican antes de entrenar un modelo de ML. Estos métodos evalúan la relación entre cada variable y la variable objetivo (o alguna medida de relevancia) sin tener en cuenta un modelo específico. Los métodos *filter* utilizan estadísticas, métricas de rendimiento, pruebas de hipótesis u otras técnicas para clasificar las características en función de su relevancia. Algunos ejemplos de métodos *filter* incluyen la correlación de Pearson, la información mutua y la prueba estadística chi-cuadrado. La principal ventaja de los métodos *filter* es su eficiencia computacional, ya que no requieren entrenar modelos.
:::

::: {.callout-note collapse="true"}
## Métodos wrapper

Los métodos *wrapper* también son técnicas de selección de características, pero al contrario que los métodos *filter*, utilizan modelos de ML as características. Se evalúan múltiples modelos mediante procedimientos que añaden y/o eliminan variables predictoras para encontrar la combinación óptima que maximice el rendimiento del modelo. Los métodos *wrapper* pueden ser más precisos que los métodos *filter*, ya que tienen en cuenta la interacción entre las características, pero tienden a ser más computacionalmente costosos, ya que involucran el entrenamiento repetido de modelos.
:::

::: {.callout-note collapse="true"}
## Métodos embedded:

Los métodos *embedded* incorporan la selección de características directamente en el proceso de entrenamiento de un modelo de ML. En lugar de realizar la selección de características como un paso separado, estos métodos evalúan la relevancia de las características mientras se ajustan al modelo. Esto significa que las características se seleccionan o ponderan automáticamente durante el entrenamiento del modelo. Ejemplos de métodos *embedded* incluyen la regresión L1 (Lasso), que impone penalizaciones a los coeficientes de las características menos importantes, y los métodos de árboles de decisión, que pueden evaluar la importancia de las características durante la construcción del árbol.
:::

La elección entre métodos *filter*, *wrapper* y *embedded* depende de la naturaleza del problema, el conjunto de datos y las necesidades específicas del análisis. Cada enfoque tiene sus propias ventajas y desventajas, y es importante considerar factores como la eficiencia computacional, la calidad del modelo y la interpretabilidad al seleccionar el enfoque adecuado para la reducción de la dimensionalidad en un proyecto de ciencia de datos o ML.

## Análisis de Componentes Principales (PCA)

Tal y como se ha indicado, el Análisis de Componentes Principales (PCA, en inglés, *Principal Component Analysis*) es una técnica de reducción de la dimensionalidad cuyo principal objetivo es simplificar la estructura de datos, preservando al mismo tiempo la mayor cantidad posible de información relevante. PCA logra esto transformando un conjunto de variables correlacionadas en un conjunto nuevo de variables no correlacionadas llamadas componentes principales.

Los pasos para aplicar esta técnica sobre un conjunto de datos son:

1.  **Cálculo de la matriz de covarianza:** El primer paso en PCA implica calcular la matriz de covarianza de las variables originales. La covarianza es una medida de cómo dos variables cambian juntas. Una matriz de covarianza muestra cómo todas las variables del conjunto de datos se relacionan entre sí.

2.  **Obtención de los componentes principales:** A continuación, se calculan los autovectores y autovalores de la matriz de covarianza. Los autovectores son las direcciones en las cuales los datos tienen la mayor varianza, y los autovalores representan la cantidad de varianza explicada por cada autovector. Probablemente hayas estudiado técnicas para el cálculo de autovectores y autovalores en cursos de Álgebra anteriores.

3.  **Selección de componentes principales:** Después de calcular los autovectores y autovalores, se ordenan en orden descendente según la cantidad de varianza que explican. Esto implica que el primer componente principal explica la mayor varianza en los datos, el segundo componente principal explica la segunda mayor varianza (una vez eliminada la variabilidad explicada por el primer componente principal), y así sucesivamente. Por lo general, se selecciona un número pequeño (¡reducción de la dimensión!) de componentes principales que capturen una cantidad significativa de la varianza total.

4.  **Transformación de datos:** Finalmente, los datos originales se transforman en el espacio de los componentes principales. Esto significa que las variables originales se combinan linealmente para formar nuevas variables (los componentes principales) que son ortogonales entre sí. Es decir, se crean nuevas variables, como combinación lineal de las originales, con la particularidad de que esas nuevas variables son incorreladas y de varianza $1$. Por lo tanto, estos componentes principales no tienen multicolinealidad, lo que es útil en análisis posteriores.

El PCA se utiliza en diversas aplicaciones, como reducción de dimensionalidad, compresión de imágenes, análisis de datos, reconocimiento de patrones, etc. Permite simplificar datos complejos mientras se retiene la mayor cantidad posible de información importante. Al seleccionar un número apropiado de componentes principales, es posible reducir la dimensionalidad de los datos sin perder significado, lo que puede mejorar la eficiencia del análisis y la visualización.

 **Profundicemos un poco más**

El PCA busca encontrar una representación más eficiente (menor dimensión) de los datos preservando la información contenida en ellos. La idea fundamental es que **la varianza de los datos representa la cantidad de información que contienen**, y PCA rota el sistema de coordenadas para encontrar las direcciones en las que la varianza es máxima.

::: {.callout-caution title="Para recordar"}
Relación entre los conceptos de varianza e información.
:::

Formalmente, dado un conjunto de datos con $p$ variables, la técnica busca transformar estas variables en un nuevo conjunto de $p$ variables ortogonales denominadas componentes principales, ordenadas de manera que capturen la mayor cantidad de varianza posible y sean ortogonales.

**Formulación matemática**

Sea $\mathbf{X}=(\mathbf{x}_1,\dots,\mathbf{x}_p) \in \mathbb{R}^{n \times p}$ una matriz de datos con $n$ observaciones y $p$ variables, centrada en la media (esto es, cada columna tiene media cero). El objetivo del PCA es encontrar un conjunto de vectores $\mathbf{W}=(\mathbf{w}_1, \mathbf{w}_2, \dots, \mathbf{w}_p)$ tales que las proyecciones de los datos sobre estos vectores maximicen la varianza (cantidad de información explicada).

Las componentes principales vienen dadas por una combinación lineal de las variables originales $\mathbf{x}_1,\dots,\mathbf{x}_p$, en el caso de la primera $\mathbf{z}_1$:

$$
\mathbf{z}_1=w_{11} \mathbf{x}_1+\cdots + w_{1p} \mathbf{x}_p
$$

o, en términos matriciales

$$
\mathbf{z}_1 = \mathbf{X}\mathbf{w}_1 .
$$

La primera componente se calcula buscando que tenga la máxima varianza posible. Evidentemente, podríamos hacer la varianza de $\mathbf{z}_1$ todo lo grande que quisiéramos escogiendo valores grandes de $w_{11},w_{12},\dots,w_{1p}$. Para evitar esto, se impone la siguiente restricción $w^{2}_{11}+w^{2}_{12}+ \cdots + w^{2}_{1p}=1,$ es decir, que el módulo del vector sea 1, $|\mathbf{w}_1| = 1.$

Así, lo que queremos hacer es calcular $\mathbf{w}_1$ que maximice la varianza de $\mathbf{z}_1$ con la restricción de que $|\mathbf{w}_1| = 1$. La varianza de $\mathbf{z}_1$ es

$$
Var(\mathbf{z}_1) = \frac{1}{n}\mathbf{z}^{t}_1 \mathbf{z}_1 = \frac{1}{n}\mathbf{w}^{t}_1 \mathbf{X}^{t} \mathbf{X} \mathbf{w}_1 = \mathbf{w}^{t}_1  \mathbf{S} \mathbf{w}_1
$$

donde $\mathbf{S} = \frac{1}{n} \mathbf{X}^{t} \mathbf{X}$ es la matriz de covarianza de los datos (definida positiva). $\mathbf{w}_1$ se obtiene resolviendo el problema de optimización:

$$
\max_{\mathbf{w}_1}  \mathbf{w}_1^{t}\mathbf{S}\mathbf{w}_1, \text{   sujeto a }\mathbf{w}_1^{t}\mathbf{w}_1 = 1,
$$

Resolver este problema mediante los multiplicadores de Lagrange (os sonará de asignaturas previas como Optimización I):

$$
L(\mathbf{w}_1, \lambda_1) = \mathbf{w}_1^{t}\mathbf{S}\mathbf{w}_1 - \lambda_1(\mathbf{w}_1^{t}\mathbf{w}_1 - 1)
$$

y maximizamos la expresión derivando con respecto a $\mathbf{w}_1$ y haciendo cero la derivada:

$$
\frac{\partial L}{\partial \mathbf{w}_1} = 2\mathbf{S}\mathbf{w}_1 - 2\lambda_1 \mathbf{w}_1=0,
$$

lo que implica que $\mathbf{S}\mathbf{w}_1 = \lambda_1 \mathbf{w}_1$.

Esto muestra que $\mathbf{w}_1$ es un autovector de la matriz de covarianza $\mathbf{S}$ y $\lambda_1$ es su autovalor asociado. ¿Qué autovalor es $\lambda_1$? Si en $\mathbf{S}\mathbf{w}_1 = \lambda_1 \mathbf{w}_1$multiplicamos por $\mathbf{w}^{t}_1$:

$$
\underbrace{\mathbf{w}^{t}_1\mathbf{S}\mathbf{w}_1}_{Var(\mathbf{z}_1)} = \lambda_1 \underbrace{\mathbf{w}^{t}_1 \mathbf{w}_1}_{1} \Rightarrow Var(\mathbf{z}_1) = \lambda_1
$$

Como lo que se quiere maximizar es la varianza, la primera componente principal se obtiene escogiendo $\lambda_1$ como el mayor autovalor de $\mathbf{S}$. Su autovector $\mathbf{w}_1$ nos indica los coeficientes asociados a cada una de las variables $\mathbf{x}_j$ originales para lograr la combinación lineal $\mathbf{z}_1$.

Similarmente, se obtiene la segunda componente principal. Ahora buscamos maximizar la suma de las varianzas de $\mathbf{z}_1$ y $\mathbf{z}_2$. En este caso la función de Lagrange a optimizar queda

$$
L(\mathbf{w}_2, \lambda_2) = \mathbf{w}_1^{t}\mathbf{S}\mathbf{w}_1 +\mathbf{w}_2^{t}\mathbf{S}\mathbf{w}_2 - \lambda_1(\mathbf{w}_1^{t}\mathbf{w}_1 - 1) - \lambda_2(\mathbf{w}_2^{t}\mathbf{w}_2 - 1)
$$

Como antes, derivando e igualando a 0:

$$
\frac{\partial L}{\partial \mathbf{w}_1} = 2\mathbf{S}\mathbf{w}_1 - 2\lambda_1 \mathbf{w}_1=0,
$$

$$
\frac{\partial L}{\partial \mathbf{w}_2} = 2\mathbf{S}\mathbf{w}_2 - 2\lambda_2 \mathbf{w}_2=0,
$$

De donde se obtiene que $\mathbf{S}\mathbf{w}_1 = \lambda_1 \mathbf{w}_1$ y que $\mathbf{S}\mathbf{w}_2 = \lambda_2 \mathbf{w}_2$*.* Esto nos vuelve a llevar a que$\mathbf{w}_1$ y $\mathbf{w}_2$ son autovectores de la matriz de covarianzas con $\lambda_1$ y $\lambda_2$ como autovalores asociados. Como antes, se escogerán los autovalores de mayor valor puesto que son los que maximizan la varianza. Además, estos dos autovectores están incorrelados (son ortogonales) dado que

$$
\mathbf{w}_1^{t} \lambda_2 \mathbf{w}_2=\mathbf{w}_1^{t} \mathbf{S}\mathbf{w}_2 \underbrace{=}_{\mathbf{S} \text{ simétrica}} \mathbf{w}_2^{t} \mathbf{S}\mathbf{w}_1 =\mathbf{w}_2^{t} \lambda_1\mathbf{w}_1
$$

y como $\lambda_1 \neq \lambda_2 \Rightarrow \mathbf{w}_1^{t}\mathbf{w}_2= \mathbf{w}_2^{t}\mathbf{w}_1=0$, esto es, son vectores ortogonales (como además tienen norma 1, son ortonormales). Esto significa que las componentes principales $\mathbf{z}_1$ y $\mathbf{z}_2$ están incorreladas.

Procediendo del mismo modo se irían consiguiendo el resto de componetes principales. De forma general, se obtiene que las soluciones al problema de optimización son los autovectores $\mathbf{w}_1, \mathbf{w}_2, \mathbf{w}_3, \dots, \mathbf{w}_p$ asociados a los $p$ autovalores $\mathbf{\lambda}=(\lambda_1,\dots,\lambda_p)$ de $\mathbf{S}$, en orden descendente. Sin pérdida de generalidad asumimos que $\lambda_1>\cdots>\lambda_p$ para que las componentes principales estén ordenadas por cantidad de varianza explicada.

Como las nuevas variables calculadas están incorreladas, la matriz de covarianzas de $\mathbf{Z}$ es

$$
\text{Var}(\mathbf{Z}) =\mathbf{S}_Z
=
\begin{bmatrix}
\lambda_1 & 0 & \dots & 0 \\
0 & \lambda_2 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots & \lambda_p
\end{bmatrix}
$$

Realmente, con PCA, lo que hemos hecho ha sido diagonalizar la matriz de covarianza $\mathbf{S}$ (esto equivale a encontrar una base ortonormal de autovectores en la que la matriz de covarianza sea diagonal):

$$
\mathbf{S}_Z=\text{Var}(\mathbf{Z}) = \mathbf{W}^{t}\text{Var}(\mathbf{X})\mathbf{W} = \mathbf{W}^{t}\mathbf{S}\mathbf{W}
$$

Generalmente la matriz de covarianza $\mathbf{S}$ es definida positiva (puede ser semidefinida positiva), es decir, tiene $p$ autovalores positivos. Por tanto, tendremos el mismo número de componentes principales que de variables originales:

$$
\mathbf{Z} = \mathbf{X}\mathbf{W}
$$

En resumen, calcular las componentes principales es realizar una transformación ortogonal $\mathbf{W}$ a las variables $\mathbf{X}$ (ejes originales) logrando nuevas variables $\mathbf{Z}$ que están incorreladas entre sí y que maximizan la varianza. Las nuevas variables $\mathbf{Z}$ son una combinación lineal de las originales $\mathbf{X}$ por lo que nos ayudaremos de los valores de los coeficientes $\mathbf{W}$ para interpretarlas y darles un significado.

**Porcentaje de variabilidad**

Tenemos que

$$
\text{Var}(\mathbf{Z}) = \sum_{i=1}^{p} \text{Var}(\mathbf{z}_i)=\sum_{i=1}^{p} \lambda_i =\text{traza}(\mathbf{S}_Z)
$$

Por las propiedades de la traza, $\text{traza}(\mathbf{S}_Z) = \text{traza}(\mathbf{S})=\sum_{i=1}^{p} \text{Var}(\mathbf{x}_i)$

Es decir, la suma de las varianzas de las variables originales y de las componentes principales coincide. Con ello, podemos especificar el porcentaje de varianza que explica cada componente principal:

$$
\frac{\lambda_i}{\sum_{i=1}^{p} \lambda_i}=\frac{\lambda_i}{\sum_{i=1}^{p} \text{Var}(\mathbf{x}_i)}
$$

Del mismo modo, se puede calcular el porcentaje de variabilidad explicada por las primeras componentes.

**Interpretación geométrica**

El procedimiento descrito equivale a realizar una rotación del sistema de coordenadas original a un nuevo sistema donde los ejes están alineados con las direcciones de máxima varianza. Los nuevos ejes (componentes principales) son ortogonales entre sí y capturan la mayor cantidad de información posible en las primeras componentes, permitiendo reducir la dimensionalidad reteniendo la información más relevante.

Mediante la siguiente simulación en R podemos ver la transformación que se logra tras aplicar PCA al conjunto de datos:

```{r}
#| label: interpret1
#| include: true
#| warning: false
#| echo: true
#| code-fold: true
#| code-summary: "Click para ver el código"
# Ejemplo transformación ortonormal PCA

# Librerías
library(ggplot2)
library(gridExtra)

# Semilla para reproducibilidad
set.seed(42)

# Datos correlacionados
n <- 100
x1 <- rnorm(n, mean = 0, sd = 2)
x2 <- 0.8 * x1 + rnorm(n, mean = 0, sd = 1)
data <- data.frame(x1, x2)

# PCA
pca <- prcomp(data, center = TRUE, scale. = TRUE)

# Guardar nuevas varialbes
data_pca <- as.data.frame(pca$x)
colnames(data_pca) <- c("PC1", "PC2")

# Gráfico datos originales
p1 <- ggplot(data, aes(x1, x2)) +
  geom_point(color = "blue", alpha = 0.6) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_fixed() +
  ggtitle("Datos originales")

# Gráfico componentes
p2 <- ggplot(data_pca, aes(PC1, PC2)) +
  geom_point(color = "red", alpha = 0.6) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_fixed(ratio=1.5) +
  ggtitle("Datos transformados (PCA)")

grid.arrange(p1, p2, ncol = 2)

```

### PCA en R

Trabajemos ahora con un ejemplo de datos deportivos reales. Utilizaremos estadísticas de partidos de tenis femenino (WTA), donde cada observación corresponde a un partido, y las variables recogen información básica de las jugadoras implicadas.

En concreto, consideraremos las siguientes variables cuantitativas:

- `winner_rank`: ranking de la jugadora ganadora

- `loser_rank`: ranking de la jugadora perdedora

- `winner_age`: edad de la ganadora

- `loser_age`: edad de la perdedora

- `rank_diff`: diferencia de ranking entre perdedora y ganadora

Nota que estas variables contienen información relevante, pero presentan escalas muy diferentes (ranking vs edad).

```{r}
# Leemos y preparamos los datos 

library(tidyverse)

url_matches <- "https://raw.githubusercontent.com/JeffSackmann/tennis_wta/master/wta_matches_2023.csv"
tennis <- read_csv(url_matches, show_col_types = FALSE)

df <- tennis %>%
  select(winner_rank, loser_rank, winner_age, loser_age) %>%
  drop_na() %>%
  mutate(rank_diff = loser_rank - winner_rank)

head(df)
summary(df)

```

Aplicamos PCA directamente sobre las variables:

```{r}
prcomp(df)
```

Las *standard deviations* son los autovalores de la matriz de correlaciones, y representan la variabilidad en cada componente. A mayor valor, más relevante es la variable correspondiente a efectos de visualización. Si queremos visualizar la importancia relativa de cada componente, haremos lo siguiente:

```{r}
plot(prcomp(df))
```

De modo numérico:

```{r}
summary(prcomp(df))
```

Observamos que la primera componente principal explica la mayor parte de la variabilidad. Si inspeccionamos la matriz de rotaciones:

```{r}
prcomp(df)$rotation
```

vemos que esta primera componente está dominada casi exclusivamente por las variables relacionadas con el ranking, mientras que las variables de edad tienen un peso mucho menor.

Esto no es sorprendente: los valores de `winner_rank` y `loser_rank` pueden alcanzar varios cientos, mientras que la edad suele oscilar en un rango mucho más reducido (aproximadamente entre 15 y 40 años). Como consecuencia, PCA prioriza las variables con mayor magnitud numérica.

¿Tiene sentido que una variable sea más influyente únicamente por estar medida en una escala mayor? Desde el punto de vista del análisis deportivo, no necesariamente.


Para evitar este efecto, repetimos el análisis estandarizando las variables:
```{r}
plot(prcomp(df, scale = TRUE))
summary(prcomp(df, scale = TRUE))

```

Ahora observamos que:

- La variabilidad se reparte de forma más equilibrada entre las componentes.

- Las dos primeras componentes explican una proporción sustancial de la variabilidad total.

- El análisis ya no está dominado exclusivamente por el ranking.


Antes de ir al gráfico, analicemos la matriz de rotaciones, en busca de interpretación semántica para las componentes principales:

```{r}
prcomp(df, scale = TRUE)$rotation
```

En este caso, la primera componente principal puede interpretarse como una combinación global de nivel competitivo (ranking) y experiencia (edad), mientras que la segunda componente introduce contrastes entre edad y ranking. No obstante, esta interpretación no es única ni garantizada.

::: {.callout-important title="Para recordar"}
PCA no garantiza interpretabilidad semántica. La interpretación de las componentes debe hacerse con cautela y, siempre que sea posible, con apoyo de conocimiento del dominio deportivo.
:::

Proyectamos los partidos sobre las dos primeras componentes principales:

```{r}
plot(prcomp(df, scale = TRUE)$x[, 1:2])
```

Cada punto representa un partido. Puntos cercanos indican partidos con perfiles similares en términos de ranking y edad de las jugadoras involucradas.

Podemos mejorar el gráfico utilizando un biplot, que incorpora información de las variables originales:

¿Cómo mejorar el gráfico? Un modo posible es incorporar la información de las variables utilizando la técnica del *biplot*.

```{r}
biplot(prcomp(df, scale = TRUE))
```

En este gráfico:

Las flechas representan las variables originales.

- Su dirección y longitud indican cómo contribuyen a las componentes principales.

- Los partidos se distribuyen según estas combinaciones latentes.

## Interpretación y limitaciones

Este análisis permite:

- Detectar patrones globales en los partidos.

- Identificar partidos con perfiles extremos (grandes diferencias de ranking, jugadoras muy jóvenes o muy veteranas).

- Reducir la complejidad visual del conjunto de datos.

Sin embargo, es fundamental recordar que:

- PCA es una técnica no supervisada.

- No utiliza información sobre el resultado del partido más allá de las variables incluidas.

- Las componentes no representan conceptos deportivos “puros” de forma automática.

Por tanto, PCA debe entenderse como una herramienta exploratoria, útil para visualizar y resumir datos, pero no como un sustituto del análisis específico del rendimiento o del conocimiento experto.

::: callout-caution
## Pregunta

¿Qué ocurre cuando se realiza un PCA con variables independientes?
:::

## Selección de características

En el tema @sec-eda vimos como es posible evaluar la correlación entre una variable objetivo y las variables explicativas asociadas. A continuación, a modo de ejercicio, vamos a estudiar la relación existente entre la variable objetivo `deposit` y la variable `age` de la base de datos `bank`. Tratamos de responder a la pregunta de si la variable edad del cliente influye, o no, en si el cliente suscribirá (o no) un depósito a plazo.

```{r}
#| include: true
#| warning: false
#| echo: true

#library (ggplot2)
#ggplot(bank, aes(x = log(age), colour = deposit)) +
 # geom_density(lwd=2, linetype=1)

```

Aprentemente, no se observa una relación significativa entre ambas variables.

```{r}
#| include: true
#| warning: false
#| echo: true
# df = bank %>% 
  #    select(age,deposit)%>%
  #    mutate(log.age=log(age))

# Resumen para los casos de depósito
#summary(df %>% filter(deposit=="yes") %>% .$log.age)

# Resumen para los casos de no depósito
# summary(df %>% filter(deposit=="no") %>% .$log.age)

```

Efectivamente, los valores de resumen de edad en ambas categorías de la variable respuesta son muy similares. Gráficamente, podemos comparar los boxplots.

```{r}
#| include: true
#| warning: false
#| echo: true
# ggplot(df, aes(deposit, log.age)) +
#        geom_boxplot()

```

Podemos constrastar la hipótesis nula de igualdad de medias mediante un test de la T:

```{r}
#| include: true
#| warning: false
#| echo: true
# t.test(log.age ~ deposit, data = df)
```

El $p-valor$ es mayor que $0.10$ por lo que no hay evidencia estadística en las observaciones en contra de la hipótesis nula. Podemos concluir, por tanto, que no existe una relación significativa (o al menos aún no la hemos localizado) entre las variables `deposit` y `age`. Podríamos, por tanto, eliminar la varible explicativa de la base de datos. De este modo se reduce la dimensionalidad del problema. Sin embargo, es una práctica peligrosa. La variable `age` podría resultar significativamente relevante cuando se controlen los efectos de otras variables dentro del futuro modelo de ML.

::: callout-important
## Para recordar

Eliminar variables explicativas en etapas tempranas del análisis para reducir la dimensionalidad del problema, conlleva el riesgo de pérdida de información que podría ser útil en etapas posteriores.
:::
