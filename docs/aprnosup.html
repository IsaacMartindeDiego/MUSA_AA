<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Aprendizaje no supervisado – Aprendizaje Automático</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./eval.html" rel="next">
<link href="./reddim.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Buscar"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./aprnosup.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aprendizaje no supervisado</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Buscar" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Aprendizaje Automático</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Buscar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefacio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introducción</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Datos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Análisis Exploratorio de Datos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reddim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Técnicas de reducción de la dimensionalidad</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./aprnosup.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aprendizaje no supervisado</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eval.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Medidas de rendimiento</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./aprsup.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Aprendizaje supervisado</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reglas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Reglas de asociación</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nuevas.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Nuevas tendencias</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusiones.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Conclusiones</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliografía</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#aprendizaje-basado-en-semejanza" id="toc-aprendizaje-basado-en-semejanza" class="nav-link active" data-scroll-target="#aprendizaje-basado-en-semejanza"><span class="header-section-number">5.1</span> Aprendizaje basado en semejanza</a>
  <ul class="collapse">
  <li><a href="#medida-de-similitud-y-disimilitud" id="toc-medida-de-similitud-y-disimilitud" class="nav-link" data-scroll-target="#medida-de-similitud-y-disimilitud"><span class="header-section-number">5.1.1</span> <strong>Medida de similitud y disimilitud</strong></a></li>
  <li><a href="#ejemplos-de-métricas" id="toc-ejemplos-de-métricas" class="nav-link" data-scroll-target="#ejemplos-de-métricas"><span class="header-section-number">5.1.2</span> <strong>Ejemplos de métricas</strong></a></li>
  </ul></li>
  <li><a href="#parámetros-de-un-modelo-de-ml" id="toc-parámetros-de-un-modelo-de-ml" class="nav-link" data-scroll-target="#parámetros-de-un-modelo-de-ml"><span class="header-section-number">5.2</span> Parámetros de un modelo de ML</a></li>
  <li><a href="#clustering-no-jerárquico" id="toc-clustering-no-jerárquico" class="nav-link" data-scroll-target="#clustering-no-jerárquico"><span class="header-section-number">5.5</span> Clustering no jerárquico</a>
  <ul class="collapse">
  <li><a href="#k-medias" id="toc-k-medias" class="nav-link" data-scroll-target="#k-medias"><span class="header-section-number">5.5.1</span> <span class="math inline">\(k\)</span>-medias</a></li>
  <li><a href="#k-medias-en-r" id="toc-k-medias-en-r" class="nav-link" data-scroll-target="#k-medias-en-r"><span class="header-section-number">5.7.1</span> <span class="math inline">\(k\)</span>-medias en R</a></li>
  <li><a href="#número-óptimo-de-clústeres" id="toc-número-óptimo-de-clústeres" class="nav-link" data-scroll-target="#número-óptimo-de-clústeres"><span class="header-section-number">5.7.2</span> Número óptimo de clústeres</a></li>
  </ul></li>
  <li><a href="#cluster-jerárquico" id="toc-cluster-jerárquico" class="nav-link" data-scroll-target="#cluster-jerárquico"><span class="header-section-number">5.8</span> Cluster Jerárquico</a>
  <ul class="collapse">
  <li><a href="#cluster-jerárquico-aglomerativo" id="toc-cluster-jerárquico-aglomerativo" class="nav-link" data-scroll-target="#cluster-jerárquico-aglomerativo"><span class="header-section-number">5.8.1</span> Cluster jerárquico aglomerativo</a></li>
  <li><a href="#cluster-jerárquico-divisivo" id="toc-cluster-jerárquico-divisivo" class="nav-link" data-scroll-target="#cluster-jerárquico-divisivo"><span class="header-section-number">5.8.2</span> Cluster jerárquico divisivo</a></li>
  <li><a href="#cluster-jerárquico-en-r" id="toc-cluster-jerárquico-en-r" class="nav-link" data-scroll-target="#cluster-jerárquico-en-r"><span class="header-section-number">5.8.3</span> Cluster jerárquico en R</a></li>
  </ul></li>
  <li><a href="#mapas-auto-organizados" id="toc-mapas-auto-organizados" class="nav-link" data-scroll-target="#mapas-auto-organizados"><span class="header-section-number">5.9</span> Mapas auto-organizados</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-aprnosup" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aprendizaje no supervisado</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>En el análisis de datos deportivos es muy habitual enfrentarse a conjuntos de datos <strong>sin una etiqueta clara o única</strong>. A diferencia del aprendizaje supervisado, donde se dispone de una variable objetivo bien definida (por ejemplo, ganar/perder, lesión sí/no, victoria/derrota), en muchos problemas reales del deporte <strong>no existe una “respuesta correcta” previa</strong>. En estos casos entran en juego los modelos de <strong>aprendizaje no supervisado</strong>.</p>
<p>El aprendizaje no supervisado se centra en descubrir estructura, patrones o regularidades en los datos a partir de medidas de similitud entre observaciones, sin necesidad de clases predefinidas. En el contexto deportivo, esto permite, por ejemplo, identificar perfiles de jugadores, tipologías de acciones, estilos de juego, patrones de carga de entrenamiento o comportamientos tácticos recurrentes, sin imponer categorías a priori.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Para recordar">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Para recordar
</div>
</div>
<div class="callout-body-container callout-body">
<p>El objetivo de los algoritmos de agrupamiento es <strong>particionar</strong> el conjunto de datos en grupos de observaciones, de forma que las observaciones dentro de un mismo grupo sean lo más similares posible entre sí, y lo más diferentes posible de las observaciones de otros grupos, según una medida de similitud definida.</p>
</div>
</div>
<p>A estos grupos se les denomina <strong>clústeres</strong> o <strong>conglomerados</strong>. La idea fundamental del clustering es que dos observaciones que comparten un clúster representan comportamientos deportivos similares, mientras que observaciones en clústeres distintos representan comportamientos claramente diferenciables.</p>
<p>En deporte, las técnicas de agrupamiento se utilizan, entre otros fines, para:</p>
<ul>
<li>segmentar jugadores según su <strong>perfil de rendimiento</strong>,</li>
<li>identificar <strong>estilos de juego</strong> en equipos o deportistas individuales,</li>
<li>detectar <strong>anomalías</strong> (rendimientos atípicos, cargas inusuales, posibles errores de medición),</li>
<li>reducir la complejidad de grandes volúmenes de datos procedentes de tracking o sensores,</li>
<li>apoyar procesos posteriores de modelización supervisada.</li>
</ul>
<p>El clustering se encuadra dentro del aprendizaje basado en similitud (<em>similarity-based learning</em>), y puede proporcionar una primera visión de la <strong>estructura latente</strong> de los datos antes de formular hipótesis más concretas o definir variables objetivo.</p>
<div class="callout callout-style-default callout-caution callout-titled" title="Atención">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Atención
</div>
</div>
<div class="callout-body-container callout-body">
<p>En muchos proyectos de analítica deportiva es recomendable comenzar con técnicas de agrupamiento, incluso cuando posteriormente se disponga de etiquetas y el problema pueda abordarse mediante aprendizaje supervisado.</p>
</div>
</div>
<p>Desde el punto de vista metodológico, existe una amplia variedad de algoritmos de agrupamiento. Una revisión detallada puede encontrarse en <span class="citation" data-cites="xu2015comprehensive">(<a href="references.html#ref-xu2015comprehensive" role="doc-biblioref">Xu y Tian 2015</a>)</span>. Entre los más utilizados destacan:</p>
<ul>
<li>los métodos basados en <strong>centroides</strong>, como el algoritmo de las <em>k-medias</em>, habituales en la segmentación de jugadores;</li>
<li>los métodos basados en <strong>conectividad</strong>, como el <em>agrupamiento jerárquico</em>, útiles para explorar relaciones anidadas entre perfiles;</li>
<li>los métodos basados en <strong>densidad</strong>, como <em>DBSCAN</em>, especialmente relevantes para la detección de comportamientos atípicos o eventos raros.</li>
</ul>
<p>En ocasiones, los algoritmos de agrupamiento se clasifican de forma general en <strong>jerárquicos</strong> y <strong>no jerárquicos</strong>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Agrupamiento jerárquico">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Agrupamiento jerárquico
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>El agrupamiento jerárquico genera una estructura de clústeres <strong>anidados</strong>, donde las observaciones se van agrupando progresivamente. Una vez que una observación se integra en un clúster, no lo abandona, sino que este clúster puede unirse a otros en niveles superiores. En deporte, este enfoque resulta útil para explorar jerarquías de perfiles o estilos.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Agrupamiento no jerárquico">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Agrupamiento no jerárquico
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>El agrupamiento no jerárquico produce una partición directa del conjunto de datos en clústeres <strong>no superpuestos</strong>, sin relaciones jerárquicas entre ellos. Este enfoque es habitual cuando se desea una segmentación clara y operativa, por ejemplo, para clasificar jugadores en perfiles funcionales bien definidos.</p>
</div>
</div>
</div>
<p>El aprendizaje no supervisado es una herramienta poderosa, pero también presenta desafíos importantes: la elección de la medida de similitud, el número de clústeres, la estabilidad de las soluciones y, especialmente en deporte, la <strong>interpretación semántica</strong> de los grupos obtenidos. A lo largo de este tema analizaremos las principales técnicas de clustering, sus propiedades y sus limitaciones, siempre desde una perspectiva crítica y aplicada al análisis de datos deportivos.</p>
<section id="aprendizaje-basado-en-semejanza" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="aprendizaje-basado-en-semejanza"><span class="header-section-number">5.1</span> Aprendizaje basado en semejanza</h2>
<p>El aprendizaje basado en similitud es una estrategia en la que la relación entre observaciones se fundamenta en la semejanza de sus características. Para ello, es fundamental definir un espacio de características adecuado y establecer una métrica que permita cuantificar la semejanza o desemejanza entre observaciones.</p>
<p>Cada variable descriptiva de un conjunto de datos representa una dimensión en un espacio <span class="math inline">\(m\)</span>-dimensional. Como explica <span class="citation" data-cites="kelleher2020fundamentals">(<a href="references.html#ref-kelleher2020fundamentals" role="doc-biblioref">Kelleher, Mac Namee, y D’arcy 2020</a>)</span> “<em>A feature space is an abstract</em> <span class="math inline">\(m\)</span><em>-dimensional space that is created by making each descriptive feature in the dataset an axis of an</em> <span class="math inline">\(m\)</span><em>-dimensional coordinate system and mapping each observation in the dataset to a point in this coordinate system based on the values of its descriptive features.</em>” Es decir, un <strong>espacio de características</strong> es un espacio abstracto en el que cada característica del conjunto de datos define un eje en un sistema de coordenadas de <span class="math inline">\(m\)</span> dimensiones. Así, cada observación se mapea a un punto en este espacio de acuerdo con los valores de sus características descriptivas.</p>
<p>Algunas consideraciones clave sobre este espacio de características:</p>
<ul>
<li><p>Si dos observaciones tienen exactamente los mismos valores en sus variables descriptivas, estarán representadas por el mismo punto en el espacio de características.</p></li>
<li><p>A medida que las diferencias entre las características descriptivas de dos observaciones aumentan, también lo hace la distancia entre sus puntos correspondientes en el espacio de características.</p></li>
<li><p>La distancia (¿qué distancia?) entre dos puntos del espacio de características es una medida útil de la similitud de las características descriptivas de las dos observaciones</p></li>
</ul>
<p><strong>Importancia de elegir un espacio de características adecuado</strong></p>
<p>El espacio de características y su representación son elementos cruciales en cualquier tarea de Machine Learning, ya que afectan directamente el rendimiento del modelo. En particular:</p>
<ul>
<li><p>La forma en que se representan las observaciones influye en la calidad de los resultados obtenidos.</p></li>
<li><p>La comparación entre observaciones es esencial para la clasificación y el agrupamiento.</p></li>
<li><p>Para representar la relación entre observaciones es necesario definir una métrica en el espacio de características</p></li>
<li><p>No existe una única forma de medir la semejanza o desemajanza entre observaciones, por lo que se deben elegir medidas apropiadas según el problema.</p></li>
</ul>
<p>Para definir estas relaciones, se emplean <strong>medidas de similitud, disimilitud o distancias</strong>, las cuales permiten cuantificar qué tan parecidas o diferentes son dos observaciones.</p>
<section id="medida-de-similitud-y-disimilitud" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="medida-de-similitud-y-disimilitud"><span class="header-section-number">5.1.1</span> <strong>Medida de similitud y disimilitud</strong></h3>
<p>En el análisis de conglomerados (clustering), las observaciones se agrupan en función de su semejanza. Para ello, se utilizan medidas que pueden ser de <strong>similitud</strong> o <strong>disimilitud</strong>, dependiendo de si queremos cuantificar cómo de parecidas o de distintas son dos observaciones.</p>
<p>La medida de desemejanza entre dos observaciones <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span> es una función <span class="math inline">\(\delta(x,y)\)</span> que cumple las siguientes propiedades</p>
<p>- Coincidencia: <span class="math inline">\(\delta(x,y) = 0 \Leftrightarrow x=y\)</span></p>
<p>- No negatividad: <span class="math inline">\(\delta(x,y) \geq 0\)</span></p>
<p>- Simetría: <span class="math inline">\(\delta(x,y) = \delta(y,x)\)</span></p>
<p>Si, además, la función <span class="math inline">\(\delta(x,y)\)</span> verifica la desigualdad triangular se trataría de una distancia (métrica):</p>
<p><span class="math display">\[
\delta(x,y) \leq \delta(x,z) + \delta(y,z)
\]</span></p>
<!--# Si $s$ es una semejanza, mediante la transformación $\delta^{2}_{ij} = s_{ii}+s_{jj}-2s_{ij}$ se logra una distancia $\delta$. -->
<p>En términos intuitivos, la medida de desemejanza o distancia refleja el grado de diferencia entre dos observaciones:</p>
<ul>
<li><p>A menor distancia, mayor parecido entre las observaciones.</p></li>
<li><p>A mayor distancia, menor similitud entre las observaciones.</p></li>
</ul>
<p>El tipo de métrica utilizada puede influir en la calidad del análisis de agrupamiento, por lo que es esencial elegir una métrica apropiada según la naturaleza de los datos.</p>
<p>En la siguiente sección abordaremos los distintos tipos de métricas de distancia utilizadas en clustering. <!--#  y sus implicaciones en el análisis de datos --></p>
</section>
<section id="ejemplos-de-métricas" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="ejemplos-de-métricas"><span class="header-section-number">5.1.2</span> <strong>Ejemplos de métricas</strong></h3>
<p>El éxito de los algoritmos de clustering depende en gran medida de la elección de la medida de desemejenza o de la métrica de distancia. La manera en que medimos la similitud entre puntos determina cómo se formarán los grupos, afectando su interpretación y utilidad. En este capítulo exploraremos distintas métricas según la naturaleza de los datos: continuos, categóricos o mixtos.</p>
<p><strong>Distancias para datos continuos</strong></p>
<p>Las variables continuas toman valores en <span class="math inline">\(\mathbb{R}\)</span> y pueden ser comparados mediante métricas tradicionales de distancia. A continuación se explican algunas de ellas.</p>
<ul>
<li><p><strong>Distancia Euclídea</strong> (distancia <span class="math inline">\(l_2\)</span>). Es una de las métricas más conocidas y más utilizadas. La distancia euclídea entre dos instancias <span class="math inline">\(\mathbf{x}, \mathbf{z} \in \mathbb{R}^{m}\)</span> es</p>
<p><span class="math display">\[
||\mathbf{x}-\mathbf{z}||_{2}=d_{\text{Euclídea}}(\mathbf{x},\mathbf{z}) = \sqrt{\sum_{j=1}^{m} \left(\mathbf{x}_j - \mathbf{z}_j \right)^{2}} = \left(\sum_{j=1}^{m} \left(\mathbf{x}_j - \mathbf{z}_j \right)^{2}\right)^{1/2}
\]</span></p>
<p>Esta métrica mide la distancia geométrica entre dos puntos en un espacio multidimensional. Las distancias al cuadrado enfatizan las diferencias grandes (porque las diferencias se elevan al cuadrado).</p></li>
<li><p><strong>Distancia de Manhattan</strong>. Es la distancia <span class="math inline">\(l_1\)</span>, también se conoce como distancia de bloque o distancia de taxi. La distancia de Manhattan entre dos instancias <span class="math inline">\(\mathbf{x}, \mathbf{z} \in \mathbb{R}^{m}\)</span> es</p>
<p><span class="math display">\[
||\mathbf{x}-\mathbf{z}||_{1}=d_{Manhattan}(\mathbf{x},\mathbf{z}) = \sum_{j=1}^{m} \left|\mathbf{x}_j - \mathbf{z}_j \right|
\]</span></p>
<p>Se denomina así puesto que en 2D se calcula contando cuántas filas y columnas hay que moverse horizontal y verticalmente para llegar de <span class="math inline">\(\mathbf{x}_i\)</span> a <span class="math inline">\(\mathbf{z}_i\)</span> (en contraste con la distancia euclidiana, que sería la distancia en línea recta).</p></li>
<li><p><strong>Distancia de Minkowski</strong> (distancia <span class="math inline">\(l_p\)</span>). Es una generalización de las dos anteriores, permitiendo ajustar la sensibilidad a valores extremos. La distancia de Minkowski entre dos instancias <span class="math inline">\(\mathbf{x}, \mathbf{z} \in \mathbb{R}^{m}\)</span></p>
<p><span class="math display">\[
||\mathbf{x}-\mathbf{z}||_{p}=d_{Minkowski}(\mathbf{x},\mathbf{z}) = \left(\sum_{j=1}^{m} \left(\mathbf{x}_j - \mathbf{z}_j \right)^{p}\right)^{1/p},
\]</span></p>
<p>con <span class="math inline">\(p=1,2,\dots, \infty\)</span>. En función del valor de <span class="math inline">\(p\)</span> se logran distintas métricas de distancia. En particular, cuando <span class="math inline">\(p=1\)</span> obtenemos la distancia de Manhattan y cuando <span class="math inline">\(p=2\)</span> obtenemos la distancia euclídea. A partir de ella, se pueden definir infinitas distancias. Mayores valores de <span class="math inline">\(p\)</span> dan más énfasis a las diferencias grandes que valores pequeños de <span class="math inline">\(p\)</span>, ya que todas las diferencias se elevan a la potencia de <span class="math inline">\(p\)</span>.</p>
<p>En el extremo, cuando <span class="math inline">\(p=\infty\)</span>, la métrica devuelve la diferencia máxima entre cualquiera de las variables, es decir, <span class="math inline">\(||\mathbf{x}-\mathbf{z}||_{\infty} = max_{1\leq j \leq m}|\mathbf{x}_j - \mathbf{z}_j|\)</span><em>.</em> Se conoce como la distancia de Chebyshev (distancia <span class="math inline">\(l_{\infty}\)</span>).</p></li>
</ul>
<p><strong>Distancias para datos binarios</strong></p>
<p>Cuando los datos son binarios se utilizan métricas especializadas para dicho tipo de datos para que reflejen correctamente la similitud entre las observaciones. Estas métricas se basan en las coincidencias entre los 0 y los 1 de las observaciones. En base a la siguiente tabla definimos:</p>
<ul>
<li><p><span class="math inline">\(a\)</span> es el número de variables binarias con valores simultáneos iguales a 1 para las observaciones <span class="math inline">\(i\)</span> y <span class="math inline">\(j\)</span>,</p></li>
<li><p><span class="math inline">\(b\)</span> es el número de variables con valor 1 para la observación <span class="math inline">\(j\)</span> y con valor 0 para la observación <span class="math inline">\(i\)</span>,</p></li>
<li><p><span class="math inline">\(c\)</span> es el número de variables con valor 0 para la observación <span class="math inline">\(j\)</span> y con valor 1 para la observación <span class="math inline">\(i\)</span>,</p></li>
<li><p><span class="math inline">\(d\)</span> es el número de variables binarias con valores simultáneos iguales a 0 para las observaciones <span class="math inline">\(i\)</span> y <span class="math inline">\(j\)</span>.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="apr_no_sup_images/tabla_binarias.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>En base a esto, algunas distancias para datos binarios son:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="apr_no_sup_images/distancias_binarias.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>También existe la distancia de Hamming que cuenta el número de posiciones en las que dos vectores binarios difieren:</p>
<p><span class="math display">\[
d_{Hamming}(\mathbf{x},\mathbf{z})=\sum_{j=1}^{m} I(x_j \neq z_j)
\]</span></p>
<p>siendo <span class="math inline">\(I(\cdot)\)</span> la función indicatriz. Si por ejemplo tenemos</p>
<p><span class="math display">\[
\mathbf{x}=(1,0,1,1) \ \ \ \mathbf{z}=(0,1,1,1)
\]</span></p>
<p>La distancia de Hamming es 2 porque hay dos diferencias.</p>
<p><strong>Distancias para datos categóricos</strong></p>
<p>Cuando se tienen datos categóricos nominales, es común medir la similitud entre las observaciones en términos de las frecuencias observadas en las distintas categorías (tablas de contingencia). Un ejemplo sería la <strong>distancia de Goodall</strong> <span class="citation" data-cites="goodall1966new">(<a href="references.html#ref-goodall1966new" role="doc-biblioref">Goodall 1966</a>)</span>. Otra opción para este tipo de datos es utilizar la distancia de Hamming.</p>
<p>Si las variables son ordinales, hay que tener en cuenta el orden de los datos. En estos casos hay distintas alternativas: utilizar la correlación de Spearman, pasar los datos a rangos y normalizarlos al intervalo <span class="math inline">\([0,1]\)</span> y usar distancias para datos de intervalo. <!--# Para datos ordinales: estandarizar (poner en escala [0,1] haciendo: replace an ordinal variable value by its rank r_{ij} \in \{1,2,...,M_f\}, la transformación sería z_{ij}=\frac{r_{ij}-1}{M_{f}-1}) y usar una distancia para datos continuos. Correlación de spearman. También se puedent tratar directamente como continuas. --></p>
<p><strong>Distancias para datos mixtos</strong></p>
<p>Cuando el conjunto de datos incluye variables continuas y categóricas, una posibilidad es utilizar la distancia de Gower <span class="citation" data-cites="gower1971general">(<a href="references.html#ref-gower1971general" role="doc-biblioref">Gower 1971</a>)</span>. La distancia de Gower entre dos observaciones <span class="math inline">\(\mathbf{x}\)</span> y <span class="math inline">\(\mathbf{z}\)</span> se define como</p>
<p><span class="math display">\[
d_{Gower}(\mathbf{x},\mathbf{z}) = 1-\frac{\sum_{t=1}^{m_1}(1-|x_{t}-z_{t}|/R_t) + a + a'}{m_1 + (m_2 - d) + m_3}
\]</span></p>
<p>siendo <span class="math inline">\(m_1\)</span> el número de variables continuas, <span class="math inline">\(m_2\)</span> el número de variables dicotómicas y <span class="math inline">\(m_3\)</span> el número de variables cualitativas. <span class="math inline">\(R_t\)</span> es el rango de la variable número <span class="math inline">\(t\)</span>, <span class="math inline">\(a\)</span> y <span class="math inline">\(d\)</span> se definen como en la tabla previa de distancias para datos binarios y <span class="math inline">\(a'\)</span> es el número de coincidencias para las variables cualitativas.</p>
<p>De forma general, cuando tenemos datos mixtos, podemos definir una distancia que combine distintas distancias en función del tipo de variable <span class="math inline">\(f\)</span>:</p>
<p><span class="math display">\[
d_{Mixta}(\mathbf{x},\mathbf{z})=\frac{\sum_{j=1}^{m} w_{j}^{(f)} d_{j}^{(f)}}{w_{j}^{(f)}}
\]</span></p>
<p>En esta fórmula, <span class="math inline">\(d_{j}^{(f)}\)</span> dependerá de la tipología de la variable <span class="math inline">\(f\)</span>.</p>
</section>
</section>
<section id="parámetros-de-un-modelo-de-ml" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="parámetros-de-un-modelo-de-ml"><span class="header-section-number">5.2</span> Parámetros de un modelo de ML</h2>
<p>Un parámetro es un valor que el algoritmo del modelo de ML ajusta durante el proceso de entrenamiento para hacer que el modelo se adapte mejor a los datos de entrenamiento y, en última instancia, haga predicciones más precisas en datos no vistos (datos de prueba o datos en producción). Los parámetros son esenciales para definir la estructura y el comportamiento del modelo.</p>
<p>En ocasiones se diferencia entre dos tipos de parámetros en un modelo de ML:</p>
<div class="callout callout-style-default callout-note callout-titled" title="Parámetros del modelo">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Parámetros del modelo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<section id="parámetros-del-modelo" class="level2 callout-body-container callout-body" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="parámetros-del-modelo"><span class="header-section-number">5.3</span> Parámetros del modelo</h2>
<p>Estos son los componentes internos del modelo que definen su estructura y su capacidad para representar relaciones en los datos. Por ejemplo, en una red neuronal, los pesos y sesgos en las capas de neuronas son parámetros del modelo. En una regresión lineal, los coeficientes son parámetros del modelo.</p>
</section>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Hiperparámetros del modelo">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hiperparámetros del modelo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<section id="hiperpárametros-del-modelo" class="level2 callout-body-container callout-body" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="hiperpárametros-del-modelo"><span class="header-section-number">5.4</span> Hiperpárametros del modelo</h2>
<p>A diferencia de los parámetros del modelo, los <strong>hiperparámetros</strong> son valores que se establecen antes del proceso de entrenamiento y controlan aspectos más generales del modelo. Ejemplos de hiperparámetros incluyen la tasa de aprendizaje, la cantidad de capas ocultas en una red neuronal, el valor <span class="math inline">\(k\)</span> en el modelo de <span class="math inline">\(k\)</span> vecinos, la profundidad de un árbol de decisión, etc. Los hiperparámetros afectan cómo se ajustan los parámetros del modelo durante el entrenamiento.</p>
</section>
</div>
</div>
<p>El proceso de ajuste de parámetros y hiperparámetros se realiza mediante la iteración y la experimentación para encontrar la combinación adecuada que permita al modelo aprender de manera efectiva y generalizar bien a datos nuevos. Esto se conoce como ajuste de hiperparámetros o búsqueda de hiperparámetros y es una parte crítica del desarrollo de modelos exitosos de ML.</p>
</section>
<section id="clustering-no-jerárquico" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="clustering-no-jerárquico"><span class="header-section-number">5.5</span> Clustering no jerárquico</h2>
<p>El clustering no jerárquico es una técnica de agrupamiento que tiene como objetivo asignar cada observación a un grupo o cluster en función de su desemejanza con el resto de elementos del conjunto de datos. A diferencia del clustering jerárquico, en el que los clusters se construyen de manera progresiva fusionando o dividiendo grupos, en el clustering no jerárquico se parte de una asignación inicial y se optimiza iterativamente.</p>
<p>Para aplicar este método, es necesario especificar de antemano el número de clusters <span class="math inline">\(K\)</span>, asegurando que este valor sea menor que el número total de observaciones (<span class="math inline">\(K &lt; n\)</span>). Cada cluster se etiqueta con un entero <span class="math inline">\(k \in {1, \dots, K}\)</span> y cada observación <span class="math inline">\(i \in {1, \dots, n}\)</span> se asigna de forma exclusiva a uno de los clusters.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Por amor al conocimiento">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Por amor al conocimiento
</div>
</div>
<div class="callout-body-container callout-body">
<p>¿Crees que solo se pueden lograr particiones en las que cada punto pertenece únicamente a un cluster? Te animamos a investigar un poco sobre la versión <em>fuzzy</em> del clustering.</p>
</div>
</div>
<p>Matemáticamente, esta asignación se representa mediante una función <span class="math inline">\(S(i)\)</span> que indica a qué cluster pertenece cada observación: <span class="math inline">\(k=S(i)\)</span>, que asigna la observación <span class="math inline">\(i\)</span>-ésima al <span class="math inline">\(k\)</span>-ésimo cluster.</p>
<p>El objetivo del clustering no jerárquico es que la asignación de las observaciones cumpla ciertas condiciones en función de las desemejanzas entre los puntos. En particular, se quiere minimizar una función de pérdida en base a la partición resultante.</p>
<p>¿Qué función minimizar? Dado que se busca asignar puntos similares al mismo cluster, una elección natural para la función de pérdida es minimizar la distancia entre observaciones dentro de un mismo cluster. Esto se formaliza mediante la siguiente expresión:</p>
<p><span class="math display">\[
W(S) = \sum_{k=1}^{K} \sum_{S(i)=k} \sum_{S(i')=k, i'\neq i} d(x_i,x_{i'})
\]</span></p>
<p>Esta función mide la suma de las distancias entre pares de observaciones dentro de cada cluster, conocida como <strong>varianza intra-cluster</strong> <span class="math inline">\(W(S)\)</span>. Buscamos agrupamientos que minimicen esta cantidad, asegurando que las observaciones asignadas a un mismo cluster sean lo más similares posibles.</p>
<p>Nótese que la dispersión total de los datos, <span class="math inline">\(T(X)\)</span>, puede descomponerse en la suma de la varianza intra-cluster y la varianza entre clusters <span class="math inline">\(B(S)\)</span>:</p>
<p><span class="math display">\[
    T(X) =\sum_{i=1}^{n} \sum_{i'=1,i' \neq i}^{n} d(x_i,x_{i'}) =
\]</span></p>
<p><span class="math display">\[
=\sum_{k=1}^{K} \sum_{S(i)=k} \left( \sum_{S(i')=k, i'\neq i} d(x_i,x_{i'}) + \sum_{S(i')\neq k, i'\neq i} d(x_i,x_{i'})  \right)
\]</span></p>
<p><span class="math display">\[
=  W(S) + B(S)
\]</span></p>
<p>donde:</p>
<ul>
<li><p><span class="math inline">\(W(S)\)</span> mide la dispersión dentro de los clusters</p></li>
<li><p><span class="math inline">\(B(S)\)</span> cuantifica la dispersión entre los clusters y se define como</p>
<p><span class="math display">\[
B(S) = \sum_{k=1}^{K} \sum_{S(i)=k}  \sum_{S(i')\neq k, i'\neq i} d(x_i,x_{i'})
\]</span></p>
<p>Cuanto más alejados estén los puntos en distintos clusters, mayor será <span class="math inline">\(B(S)\)</span>.</p></li>
</ul>
<p>Dado que <span class="math inline">\(T(X)\)</span> es fijo para un conjunto de datos dado, minimizar <span class="math inline">\(W(S)\)</span> equivale a maximizar <span class="math inline">\(B(S)\)</span> puesto que <span class="math inline">\(T(X) = W(S) + B(S) \Rightarrow W(S) = T(X) - B(S)\)</span>. En base a <span class="math inline">\(T(X) = W(S) + B(S)\)</span> se puede definir la <strong>contribución de la partición a la dispersión total</strong>:</p>
<p><span class="math display">\[
   \text{Contribución} = \frac{B(S)}{T(X)}
\]</span></p>
<p>Este cociente indica cuánto de la dispersión total se debe a diferencias entre clusters.</p>
<p>Con respecto a la resolución del problema que tenemos entre manos, este problema podría resolverse mediante optimización combinatoria, explorando todas las posibles asignaciones de observaciones a clusters. Sin embargo, este enfoque es computacionalmente inviable debido al enorme número de combinaciones posibles.</p>
<p>Por ello, los algoritmos de clustering no jerárquico adoptan estrategias heurísticas que examinan solo una fracción del espacio de soluciones, con el objetivo de identificar un subconjunto que pueda contener el óptimo o al menos una buena solución subóptima. La estrategia básica consiste en:</p>
<ol type="1">
<li><p>Comenzar con una partición inicial de los datos.</p></li>
<li><p>Actualizar iterativamente la asignación de las observaciones de modo que se minimice la función objetivo.</p></li>
<li><p>Detener el proceso cuando no se observe una mejora significativa en la función objetivo.</p></li>
</ol>
<p>Dependiendo del algoritmo utilizado, la forma en que se actualizan las asignaciones y los criterios de convergencia pueden variar. Algunos de los métodos más comunes incluyen <span class="math inline">\(k\)</span>-means, <span class="math inline">\(k\)</span>-medoids y clustering basado en densidad, cada uno con sus propias ventajas y desventajas según la naturaleza de los datos.</p>
<p>En conclusión, el clustering no jerárquico es una herramienta fundamental en el análisis de datos, permitiendo descubrir estructuras y patrones sin requerir una estructura predefinida. Su aplicación efectiva y resultados dependerá de la correcta elección del número de clusters, la métrica de distancia y el algoritmo de clustering aplicado.</p>
<section id="k-medias" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored"><span class="header-section-number">5.5.1</span> <span class="math inline">\(k\)</span>-medias</h3>
<p>El algoritmo de las <span class="math inline">\(k\)</span> medias es el algoritmo de ML no supervisado más utilizado para agrupar un conjunto de observaciones en un conjunto de <span class="math inline">\(k\)</span> grupos o clústeres, donde <span class="math inline">\(k\)</span> representa el número de grupos pre-especificados por el científico de datos. Diremos que <span class="math inline">\(k\)</span> es un valor del modelo de ML y que su valor ha de ser fijado (o aprendido) a lo largo del proceso de aprendizaje.</p>
<p>La idea básica consiste en definir clústeres de manera que se reduzca al máximo la variabilidad total dentro del clúster (llamada <em>within-cluster variation</em>):</p>
<p><span class="math display">\[ W(S) = \sum_{k=1}^{K}\sum_{\mathbf{x}_i \in S_k} (\mathbf{x}_i - \mathbf{c}_k)^{2}\]</span></p>
<p>siendo <span class="math inline">\(K\)</span> el número total de clusters, <span class="math inline">\(S_k\)</span> el cluster <span class="math inline">\(k\)</span>, <span class="math inline">\(x_i\)</span> cada una de las observaciones del conjunto de datos y <span class="math inline">\(\mathbf{c}_k\)</span> el centroide del cluster <span class="math inline">\(S_k\)</span> (la media de los elementos de dicho cluster). Nótese que los clústeres resultantes <span class="math inline">\(S=\{S_1,\dots,S_K\}\)</span> son disjuntos dos a dos, es decir, <span class="math inline">\(S_h \cap S_q =\emptyset, \forall h, q = 1,\dots,K, h\neq q\)</span>.</p>
<p>¿Por qué el centroide es la media de los elementos del cluster? Es el punto que minimiza las distancias</p>
<p><span class="math display">\[
W(S) = \sum_{k=1}^{K} W(S_k) = \sum_{k=1}^{K}\sum_{\mathbf{x}_i \in S_k} \sum_{j=1}^{m} (x_{ij} - c_{kj})^{2}
\]</span></p>
<p>Como <span class="math inline">\(W(S_k)\)</span> es una función cuadrática de <span class="math inline">\(\mathbf{c}_k\)</span>, para hallar el mínimo basta con derivar con respecto a <span class="math inline">\(c_{kj}\)</span></p>
<p><span class="math inline">\(\frac{\partial W(S_k, \textbf{c}_{\textbf{k}}) }{\partial c_{kj}} = \frac{\partial \sum_{\textbf{x}_{\textbf{i}} \in S_k} \sum_{j=1}^m(x_{ij}-c_{kj})^2}{\partial c_{kj}}=\frac{\partial \sum_{\textbf{x}_{\textbf{i}} \in S_k}(x_{ij}-c_{kj})^2}{\partial c_{kj}}=\)</span></p>
<p><span class="math inline">\(=-2\sum_{\textbf{x}_{\textbf{i}} \in S_k} (x_{ij}-c_{kj}) \qquad k=1,…,K, \ j \text{ fijo}\)</span></p>
<p>e igualar a 0: <span class="math inline">\(\frac{\partial W(S_k, \textbf{c}_{\textbf{k}}) }{\partial c_{kj}}=0 \Longleftrightarrow c_{kj}= \frac{1}{|S_k|}\sum_{\textbf{x}_{\textbf{i}} \in S_k}x_{ij}\)</span></p>
<p>siendo <span class="math inline">\(|S_k|\)</span> el cardinal de <span class="math inline">\(S_k\)</span>.</p>
<p>Los centroides óptimos para minimizar la función <span class="math inline">\(W(S)\)</span> es la media de cada cluster.</p>
<p>Existen varios algoritmos para entrenar un modelo de <span class="math inline">\(k\)</span>-medias. El algoritmo original puede encontrarse en <span class="citation" data-cites="hartigan1979algorithm">(<a href="references.html#ref-hartigan1979algorithm" role="doc-biblioref">Hartigan y Wong 1979</a>)</span>, y define la variabilidad total dentro del clúster como la suma de distancias Euclídeas al cuadrado entre las observaciones y el correspondiente centroide.</p>
<ol type="1">
<li>El algoritmo comienza con <span class="math inline">\(k\)</span> medias seleccionadas aleatoriamente del conjunto original de observaciones. Es decir, se escogen al azar <span class="math inline">\(k\)</span> centroides del conjunto de datos.</li>
</ol>
<div class="callout callout-style-default callout-caution callout-titled" title="Atención">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Atención
</div>
</div>
<div class="callout-body-container callout-body">
<p>No siempre se tiene información sobre qué valor es el óptimo para el parámetro <span class="math inline">\(k\)</span> en el modelo de las <span class="math inline">\(k\)</span>-medias. De hecho, en ocasiones el interés de los métodos de agrupamiento es precisamente averiguar dicho valor.</p>
</div>
</div>
<ol start="2" type="1">
<li><p>El algoritmo continúa asignando los registros de la base de datos al clúster con media más cercana. Es decir, para cada observación se busca su centroide más cercano dentro del conjunto de centrioides disponibles.</p></li>
<li><p>Una vez que todas las observaciones han sido agrupadas de acuerdo a su centroide más cercano, se recalculan los centroides de los <span class="math inline">\(k\)</span> clústeres.</p></li>
</ol>
<p>Se iteran estos dos últimos pasos hasta la convergencia de los centroides. Esto es, hasta que el valor de los centroides apenas se modifica (según un criterio de parada preestablecido, esto es, otro hiperparámetro).</p>
<p>Los clústeres están representados por su centroide, entendiendo éste como un punto de referencia.</p>
<p>Despliega los paneles siguientes para averiguar las principales ventajas y desventajas de este modelo de ML.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Ventajas">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<section id="ventajas" class="level2 callout-body-container callout-body" data-number="5.6">
<h2 data-number="5.6" class="anchored" data-anchor-id="ventajas"><span class="header-section-number">5.6</span> Ventajas</h2>
<p>Las principales ventajas del algoritmo de las <span class="math inline">\(k\)</span>-medias son su <strong>sencillez</strong> y su <strong>escalabilidad</strong> (aplicable con facilidad a grandes conjuntos de datos).</p>
</section>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Desventajas">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Desventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<section id="desventajas" class="level2 callout-body-container callout-body" data-number="5.7">
<h2 data-number="5.7" class="anchored" data-anchor-id="desventajas"><span class="header-section-number">5.7</span> Desventajas</h2>
<p>Las principales desventajas son la necesidad de elegir <span class="math inline">\(k\)</span> manualmente y la alta <strong>dependencia</strong> de los valores iniciales, las medias con las que comienza el algoritmo. Para evitar esta última desventaja se suele replicar el algoritmo varias veces con distintas inicializaciones. Además, los centroides pueden verse fuertemente influidos por <strong>valores atípicos</strong>.</p>
</section>
</div>
</div>
<p>Para solventar esta última desventaja, en ocasiones, se usan de <strong>medoides</strong> en lugar de centroides. Estos medoides, al contrario de los centroides, son obligatoriamente observaciones de la muestra. Es decir, no elegimos medias aleatorias, sino observaciones reales recogidas en la base de datos.</p>
<p>El más común de los algoritmos de <span class="math inline">\(k\)</span>-medoides es el PAM: “Partitioning Around Medoids”. Una ventaja adicional de los algoritmos basados en medoides es la interpretabilidad de los resultados. Mientras que un centroide puede no tener significado dentro de las observaciones muestrales, un medoide lo tiene por definición. El algoritmo de los <span class="math inline">\(k\)</span>-medoides es:</p>
<ol type="1">
<li><p>Inicialización. Se elige el número <span class="math inline">\(K\)</span> de clusters y se escogen al azar <span class="math inline">\(K\)</span> observaciones del conjunto de datos (medoides <span class="math inline">\(\mathbf{m}_k\)</span>). Se elige una distancia <span class="math inline">\(d\)</span>.</p></li>
<li><p>Se asigna cada observación <span class="math inline">\(\mathbf{x}_i\)</span> al medoide más cercano: <span class="math inline">\(argmin_{k } d(\mathbf{x}_i,\mathbf{m}_k)\)</span>. Así se forman los <span class="math inline">\(K\)</span> clusters <span class="math inline">\(\{S_1,\dots,S_K\}\)</span>.</p></li>
<li><p>Una vez formado los clusters, se recalculan los medoides <span class="math inline">\(\mathbf{m}_k =argmin_{\mathbf{x}_i \in S_k} \sum_{\mathbf{x'}_i \in S_k} d(\mathbf{x}_i,\mathbf{x'}_i), \forall k=1,\dots,K\)</span></p></li>
<li><p>Se repiten los pasos 2 y 3 hasta que no haya cambios</p></li>
</ol>
<p>Los clústeres están representados por su medoide, entendiendo éste como un punto de referencia</p>
<p>Estos algoritmos son <strong>no-jerárquicos</strong>, pues una observación puede cambiar de clúster durante la ejecución del mismo. Dos observaciones cualesquiera pueden pertenecer al mismo o a diferente grupo en diferentes iteraciones del algoritmo.</p>
</section>
<section id="k-medias-en-r" class="level3" data-number="5.7.1">
<h3 data-number="5.7.1" class="anchored" data-anchor-id="k-medias-en-r"><span class="header-section-number">5.7.1</span> <span class="math inline">\(k\)</span>-medias en R</h3>
<p>Para ilustrar el funcionamiento del algoritmo de las <span class="math inline">\(k-medias\)</span> en R, vamos a emplear datos reales de partidos de tenis femenino (WTA). En este caso, cada observación corresponde a un partido, y las variables describen características básicas de las jugadoras implicadas. El objetivo del análisis no es predecir un resultado, sino explorar si existen perfiles diferenciados de partidos en función de estas variables.</p>
<p>Trabajaremos con las siguientes variables cuantitativas:</p>
<ul>
<li><p>winner_rank: ranking de la ganadora</p></li>
<li><p>loser_rank: ranking de la perdedora</p></li>
<li><p>winner_age: edad de la ganadora</p></li>
<li><p>loser_age: edad de la perdedora</p></li>
<li><p>winner_rank_points</p></li>
<li><p>loser_rank_points</p></li>
</ul>
<p>Cargamos y preparamos los datos:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)   <span class="co"># manipulación de datos</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   4.0.0     ✔ tibble    3.2.1
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.0.4     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)     <span class="co"># algoritmos de clustering</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)  <span class="co"># clustering y visualización</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>url_matches <span class="ot">&lt;-</span> <span class="st">"https://raw.githubusercontent.com/JeffSackmann/tennis_wta/master/wta_matches_2023.csv"</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>tennis <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(url_matches, <span class="at">show_col_types =</span> <span class="cn">FALSE</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> tennis <span class="sc">%&gt;%</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(winner_rank, </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>         loser_rank, </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>         winner_age, </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>         loser_age,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>         winner_rank_points,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>         loser_rank_points,w_1stWon,l_1stWon)<span class="sc">%&gt;%</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">drop_na</span>() </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 8
  winner_rank loser_rank winner_age loser_age winner_rank_points
        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;
1           3         27       28.8      29.1               4691
2          11         54       27.8      24                 2417
3           3          1       28.8      21.5               4691
4          11         48       27.8      30.8               2417
5          27          6       29.1      27.4               1528
6          54        199       24        25.8                951
# ℹ 3 more variables: loser_rank_points &lt;dbl&gt;, w_1stWon &lt;dbl&gt;, l_1stWon &lt;dbl&gt;</code></pre>
</div>
</div>
<p>Como ocurre habitualmente en deporte, las variables están medidas en escalas muy diferentes (ranking frente a edad). Por ello, antes de aplicar <span class="math inline">\(k-medias\)</span> es imprescindible escalar las variables.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># escalado de todas las variables</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>df_scaled <span class="ot">&lt;-</span> <span class="fu">scale</span>(df)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df_scaled)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     winner_rank loser_rank winner_age  loser_age winner_rank_points
[1,]  -0.7356221 -0.5813466  0.6239706  0.5787289          1.4831737
[2,]  -0.6465729 -0.3777964  0.3826795 -0.5700989          0.3047666
[3,]  -0.7356221 -0.7773578  0.6239706 -1.1332498          1.4831737
[4,]  -0.6465729 -0.4230298  0.3826795  0.9616715          0.3047666
[5,]  -0.4684745 -0.7396634  0.6963579  0.1957863         -0.1559211
[6,]  -0.1679333  0.7153432 -0.5342269 -0.1646303         -0.4549277
     loser_rank_points   w_1stWon   l_1stWon
[1,]         0.2676452 -1.4008537 -0.5449552
[2,]        -0.2034104 -0.5577542 -1.0990074
[3,]         8.0698605 -0.6631416 -1.4683756
[4,]        -0.1389158 -0.4523667  0.2861231
[5,]         2.1804411  2.0769319  1.0248594
[6,]        -0.7079379 -0.4523667 -0.6372972</code></pre>
</div>
</div>
<p>El algoritmo de <span class="math inline">\(k-medias\)</span> se basa en la distancia entre observaciones. En este ejemplo utilizamos la distancia Euclídea. Podemos calcular y visualizar la matriz de distancias con ayuda del paquete <code>factoextra</code>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>df_scaled2 <span class="ot">=</span> df_scaled[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(df)[<span class="dv">1</span>],<span class="dv">300</span>),]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>distance <span class="ot">&lt;-</span> <span class="fu">get_dist</span>(df_scaled2)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_dist</span>(distance,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">gradient =</span> <span class="fu">list</span>(<span class="at">low =</span> <span class="st">"#00AFBB"</span>, <span class="at">mid =</span> <span class="st">"white"</span>, <span class="at">high =</span> <span class="st">"#FC4E07"</span>))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: `aes_string()` was deprecated in ggplot2 3.0.0.
ℹ Please use tidy evaluation idioms with `aes()`.
ℹ See also `vignette("ggplot2-in-packages")` for more information.
ℹ The deprecated feature was likely used in the factoextra package.
  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Este gráfico comienza a mostrar qué partidos son más similares entre sí (colores fríos) y cuáles presentan perfiles claramente distintos (colores cálidos), en función de ranking y edad de las jugadoras.</p>
<p>Podemos aplicar el algoritmo de las <span class="math inline">\(k\)</span>-medias con <span class="math inline">\(k=2\)</span> sin más que llamar a la función <code>kmeans</code> tal y como sigue:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>k2 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(df_scaled2, <span class="at">centers =</span> <span class="dv">2</span>, <span class="at">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(k2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>List of 9
 $ cluster     : int [1:300] 2 1 2 2 1 2 1 2 1 2 ...
 $ centers     : num [1:2, 1:8] -0.0946 0.0103 -0.1527 -0.1245 -0.1472 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:2] "1" "2"
  .. ..$ : chr [1:8] "winner_rank" "loser_rank" "winner_age" "loser_age" ...
 $ totss       : num 2308
 $ withinss    : num [1:2] 1183 730
 $ tot.withinss: num 1913
 $ betweenss   : num 395
 $ size        : int [1:2] 175 125
 $ iter        : int 1
 $ ifault      : int 0
 - attr(*, "class")= chr "kmeans"</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tarea
</div>
</div>
<div class="callout-body-container callout-body">
<p>¿Qué significa <code>nstart=25</code> en la anterior llamada a <code>kmeans</code>? (Pista: tiene que ver con la estabilidad de la solución).</p>
</div>
</div>
<p>Si imprimimos los resultados observamos que el algoritmo ha dividido los partidos en dos conglomerados, cada uno caracterizado por un centro (media) en las variables escaladas. Estos centros representan perfiles promedio de partido en cada clúster, no partidos reales concretos.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>k2 </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>K-means clustering with 2 clusters of sizes 175, 125

Cluster means:
  winner_rank loser_rank winner_age   loser_age winner_rank_points
1 -0.09459493 -0.1527497 -0.1471959 -0.16154098          0.1422593
2  0.01025417 -0.1245499  0.2423445  0.08910299         -0.1166699
  loser_rank_points   w_1stWon   l_1stWon
1       0.170616385 -0.6679593 -0.6214672
2      -0.003970181  0.9480216  0.9546795

Clustering vector:
  [1] 2 1 2 2 1 2 1 2 1 2 1 2 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 1 2 1 2 1 1 1 2 2 2
 [38] 1 2 1 1 2 1 1 1 2 1 1 1 1 2 1 1 2 2 1 1 2 2 2 2 1 1 2 1 2 2 2 1 1 2 2 1 1
 [75] 2 1 2 1 1 2 1 1 2 2 1 1 2 2 2 1 2 1 1 1 2 2 1 1 1 2 1 1 1 1 1 1 1 1 2 2 1
[112] 1 1 1 1 1 1 1 1 1 1 1 2 2 2 1 2 1 1 2 1 1 2 1 1 2 1 2 1 2 1 2 1 2 2 2 1 2
[149] 2 2 1 1 2 1 2 1 1 1 1 1 2 1 2 1 1 2 1 2 2 1 2 1 1 1 1 1 1 2 2 1 2 2 1 1 1
[186] 2 1 2 2 1 2 2 2 2 1 1 1 1 1 2 1 2 2 1 1 2 2 1 2 1 2 2 1 1 2 1 2 1 2 2 2 1
[223] 2 2 1 1 1 1 1 2 1 1 2 1 1 1 2 1 1 1 1 2 1 1 2 1 2 2 1 1 1 1 1 2 1 1 1 2 2
[260] 2 2 2 1 1 2 2 1 1 2 1 2 2 1 1 1 1 2 1 1 2 2 2 1 1 2 1 2 1 1 2 2 1 1 2 1 1
[297] 1 1 1 1

Within cluster sum of squares by cluster:
[1] 1182.8931  730.3236
 (between_SS / total_SS =  17.1 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"      </code></pre>
</div>
</div>
<p>Podemos representar gráficamente los resultados utilizando fviz_cluster. Dado que el conjunto de datos tiene más de dos variables, la función realiza internamente un PCA y proyecta los datos sobre las dos primeras componentes principales (ver <a href="reddim.html" class="quarto-xref"><span>Capítulo 4</span></a>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(k2, <span class="at">data =</span> df_scaled2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tarea
</div>
</div>
<div class="callout-body-container callout-body">
<p>¿Qué información estamos perdiendo al representar únicamente dos dimensiones del conjunto de datos? (Repasa el capítulo <a href="reddim.html" class="quarto-xref"><span>Capítulo 4</span></a> sobre reducción de la dimensionalidad).</p>
</div>
</div>
<p><strong>Interpretación en el dominio deportivo</strong></p>
<p>En el ejemplo anterior, los clústeres pueden interpretarse, de forma exploratoria, como tipologías de partidos, por ejemplo:</p>
<ul>
<li><p>partidos con grandes diferencias de ranking frente a partidos más equilibrados,</p></li>
<li><p>partidos protagonizados por jugadoras jóvenes frente a jugadoras más veteranas,</p></li>
<li><p>combinaciones de ambos factores.</p></li>
</ul>
<p>No obstante, es importante subrayar que:</p>
<ul>
<li><p><span class="math inline">\(k-medias\)</span> no utiliza información del resultado más allá de las variables incluidas,</p></li>
<li><p>los clústeres no tienen un significado deportivo “intrínseco”,</p></li>
<li><p>la interpretación requiere siempre conocimiento del dominio.</p></li>
</ul>
<p>Este tipo de análisis resulta especialmente útil como paso previo a tareas más complejas, como la segmentación de jugadoras, el análisis de estilos de juego o la construcción de modelos supervisados.</p>
</section>
<section id="número-óptimo-de-clústeres" class="level3" data-number="5.7.2">
<h3 data-number="5.7.2" class="anchored" data-anchor-id="número-óptimo-de-clústeres"><span class="header-section-number">5.7.2</span> Número óptimo de clústeres</h3>
<p>Como hemos comentado anteriormente, en numerosas ocasiones el número óptimo de clústeres de un problema no supervisado lo establece el dominio de aplicación, o más concretamente nuestra necesidad de comunicar los resultados y que estos tengan sentido y sean lo más explicables posibles.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Nombrar los grupos
</div>
</div>
<div class="callout-body-container callout-body">
<p>Nombrar los clusters en un análisis no supervisado es fundamental para dar significado a los resultados, comunicar eficazmente las conclusiones y facilitar la toma de decisiones informadas.</p>
</div>
</div>
<p>A continuación mostramos los tres métodos más populares para determinar el número óptimo de clústeres: método del <em>codo</em> , <em>silhoutte</em> y método estadístico de la <em>brecha</em> (Gap).</p>
<p><strong>Método del codo</strong></p>
<p>Recordemos que la idea básica de los métodos de división en clústeres, como <span class="math inline">\(k\)</span>-medias, es definir los clústeres de manera que se reduzca al mínimo la variación total dentro de los clústeres. Podemos calcular este valor de variación total para diferentes elecciones de <span class="math inline">\(k\)</span> y graficar dichos valores. La ubicación de un cambio de pendiente abrupto (un codo) en la figura se considera generalmente como un indicador del número apropiado de grupos. Veamos un ejemplo en R.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reproducible</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(df_scaled2, kmeans, <span class="at">method =</span> <span class="st">"wss"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.
ℹ Please use the `linewidth` argument instead.
ℹ The deprecated feature was likely used in the ggpubr package.
  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.
ℹ Please use the `linewidth` argument instead.
ℹ The deprecated feature was likely used in the ggpubr package.
  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
ℹ Please use `linewidth` instead.
ℹ The deprecated feature was likely used in the ggpubr package.
  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>En este caso parece que <span class="math inline">\(4\)</span> es una buena elección para el número óptimo de clusters. De modo qué:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>k4 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(df_scaled2, <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(k4, <span class="at">data =</span> df_scaled2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>Método de la silueta</strong></p>
<p>Podemos emplear otras técnicas (no supervisadas) para valorar la coherencia, o calidad, de los resultados de un algoritmo de agrupamiento. <strong>Silhouette</strong> es una de estas técnicas. El enfoque de la silueta media mide la calidad de una agrupación. Es decir, determina hasta qué punto cada observación se encuentra, correctamente ubicada, dentro de su agrupación. Una anchura de silueta media elevada indica una buena agrupación. El método de la silueta media calcula la silueta media de las observaciones para distintos valores de <span class="math inline">\(k\)</span>. El número óptimo de conglomerados <span class="math inline">\(k\)</span> es el que maximiza la silueta media en un rango de posibles valores de <span class="math inline">\(k\)</span>. Para cada observación <span class="math inline">\(x_i\)</span> de la base de datos calculamos la siguiente expresión:</p>
<p><span class="math display">\[s(x_i)=\frac{b(x_i)-a(x_i)}{max(a(x_i),b(x_i))},\]</span></p>
<p>donde <span class="math inline">\(a(x_i)\)</span> es la media de las distancias de la observación <span class="math inline">\(x_i\)</span> a los puntos en su propio clúster, <span class="math inline">\(b(x_i)\)</span> es la media de las distancias de <span class="math inline">\(x_i\)</span> a los puntos en su cluster más cercano (excluyendo el suyo propio). La interpretación es muy sencilla: las observaciones que “encajan” bien en el cluster al que pertenecen tienen valores altos, mientras que las observaciones que “no encajan” bien en el clúster al que han sido asignadas tienen valores pequeños o incluso negativos.</p>
<p>Es posible realizar análisis de agrupamiento con diferente número de clústeres y comparar en cada uno de esos análisis los valores de Silhoutte obtenidos.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(df_scaled2, kmeans, <span class="at">method =</span> <span class="st">"silhouette"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Con esta técnica, en nuestros datos de ejemplo, parece ser que la mejor elección es <span class="math inline">\(k\)</span> igual a <span class="math inline">\(3\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>k3 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(df_scaled2, <span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>sil <span class="ot">&lt;-</span> <span class="fu">silhouette</span>(k3<span class="sc">$</span>cluster, <span class="fu">dist</span>(df_scaled2))</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_silhouette</span>(sil)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  cluster size ave.sil.width
1       1   33          0.07
2       2  109          0.14
3       3  158          0.25</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>La interpretación del coeficiente de silueta es la siguiente: Un valor positivo significa que la observación está bien agrupada. Cuanto más se acerque el coeficiente a <span class="math inline">\(1\)</span>, mejor agrupada está la observación. Por contra, un valor negativo significa que la observación está mal agrupada. Finalmente, un valor igual a <span class="math inline">\(0\)</span> significa que la observación se encuentra entre dos conglomerados.</p>
<p>El gráfico de siluetas anterior y el coeficiente de silueta medio ayudan a determinar si la agrupación es buena o no. Si una gran mayoría de los coeficientes de silueta son positivos, significa que las observaciones están situadas en el grupo correcto.</p>
<p><strong>Gap</strong></p>
<p>El Método Gap es útil porque ofrece una forma objetiva de determinar el número de clusters sin depender de suposiciones subjetivas. Al comparar los resultados del clustering real con datos de referencia aleatorios, ayuda a evitar la sobreelección o subelección de clusters y permite tomar decisiones más informadas sobre la estructura de los datos.</p>
<p>El proceso es el siguiente:</p>
<ol type="1">
<li><p>Se aplica el algoritmo de clustering (por ejemplo, K-Means) a los datos con diferentes valores de <span class="math inline">\(k\)</span>, que representan el número de clusters que se desea evaluar. Se genera un conjunto de resultados de clustering para cada valor de <span class="math inline">\(k\)</span>.</p></li>
<li><p>Se generan conjuntos de <em>datos de referencia aleatorios</em> (datos simulados) con la misma estructura y variabilidad que los datos reales, pero sin patrones de clustering. Estos datos aleatorios se utilizan como referencia para evaluar la calidad de los clusters obtenidos en el paso anterior.</p></li>
<li><p>Se calcula el estadístico <strong>Gap</strong> para cada valor de <span class="math inline">\(k\)</span>. Este estadístico compara la dispersión de los datos reales con la dispersión de los datos aleatorios generados. Cuanto más grande sea la brecha entre estas dos dispersiones, más sólido es el clustering para ese valor de <span class="math inline">\(k\)</span>. El estadístico se calcula como la diferencia entre el logaritmo de la dispersión intra-cluster de los datos reales y el logaritmo de la dispersión intra-cluster de los datos de referencia.</p></li>
<li><p>Selección del Número Óptimo de Clusters: El valor de <span class="math inline">\(k\)</span> que maximiza el estadístico anterior se considera el número óptimo de clusters. En otras palabras, se elige el valor de <span class="math inline">\(k\)</span> donde la brecha entre los datos reales y los datos de referencia es más grande.</p></li>
</ol>
<p>Existe una función en R que realiza todo este proceso. En los datos de ejemplo:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>gap_stat <span class="ot">&lt;-</span> <span class="fu">clusGap</span>(df_scaled2, <span class="at">FUN =</span> kmeans, <span class="at">nstart =</span> <span class="dv">25</span>,</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>                    <span class="at">K.max =</span> <span class="dv">10</span>, <span class="at">B =</span> <span class="dv">50</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(gap_stat, <span class="at">method =</span> <span class="st">"firstmax"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Clustering Gap statistic ["clusGap"] from call:
clusGap(x = df_scaled2, FUNcluster = kmeans, K.max = 10, B = 50, nstart = 25)
B=50 simulated reference sets, k = 1..10; spaceH0="scaledPCA"
 --&gt; Number of clusters (method 'firstmax'): 1
          logW   E.logW       gap      SE.sim
 [1,] 5.610133 6.316750 0.7066173 0.010377207
 [2,] 5.505946 6.203101 0.6971553 0.010022724
 [3,] 5.436490 6.140161 0.7036713 0.010325501
 [4,] 5.385118 6.089913 0.7047942 0.010192019
 [5,] 5.335603 6.050160 0.7145577 0.010138984
 [6,] 5.295725 6.014819 0.7190949 0.010134833
 [7,] 5.248451 5.984414 0.7359622 0.010115564
 [8,] 5.216971 5.957133 0.7401621 0.009631836
 [9,] 5.190172 5.932992 0.7428203 0.009402324
[10,] 5.161018 5.911239 0.7502209 0.009817541</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_gap_stat</span>(gap_stat)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Supongamos que <span class="math inline">\(4\)</span> es el número óptimo de clusters.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>k4 <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(df_scaled2, <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">25</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(k4, <span class="at">data =</span> df_scaled2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tarea
</div>
</div>
<div class="callout-body-container callout-body">
<p>El paquete <code>NbClust</code> proporciona <strong>30 (treinta!!!)</strong> índices para determinar el número relevante de clústeres y propone a los usuarios el mejor esquema de agrupación a partir de los diferentes resultados obtenidos variando todas las combinaciones de número de clústeres, medidas de distancia y métodos de agrupación. ¡Pruébalo!</p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"NbClust"</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>nb <span class="ot">&lt;-</span> <span class="fu">NbClust</span>(df_scaled2, <span class="at">distance =</span> <span class="st">"euclidean"</span>, <span class="at">min.nc =</span> <span class="dv">2</span>,</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">max.nc =</span> <span class="dv">10</span>, <span class="at">method =</span> <span class="st">"kmeans"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>*** : The Hubert index is a graphical method of determining the number of clusters.
                In the plot of Hubert index, we seek a significant knee that corresponds to a 
                significant increase of the value of the measure i.e the significant peak in Hubert
                index second differences plot. 
 </code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-13-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>*** : The D index is a graphical method of determining the number of clusters. 
                In the plot of D index, we seek a significant knee (the significant peak in Dindex
                second differences plot) that corresponds to a significant increase of the value of
                the measure. 
 
******************************************************************* 
* Among all indices:                                                
* 5 proposed 2 as the best number of clusters 
* 6 proposed 3 as the best number of clusters 
* 2 proposed 5 as the best number of clusters 
* 6 proposed 6 as the best number of clusters 
* 1 proposed 7 as the best number of clusters 
* 1 proposed 9 as the best number of clusters 
* 2 proposed 10 as the best number of clusters 

                   ***** Conclusion *****                            
 
* According to the majority rule, the best number of clusters is  3 
 
 
******************************************************************* </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(parameters)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>n_clust <span class="ot">&lt;-</span> <span class="fu">n_clusters</span>(<span class="fu">as.data.frame</span>(df_scaled2),</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">package =</span> <span class="fu">c</span>(<span class="st">"easystats"</span>, <span class="st">"NbClust"</span>, <span class="st">"mclust"</span>),</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">standardize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>n_clust</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># Method Agreement Procedure:

The choice of 2 clusters is supported by 6 (21.43%) methods out of 28 (Silhouette, Ch, Duda, Pseudot2, Beale, Mcclain).</code></pre>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(n_clust)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Creemos que te ha quedado claro que no existe una regla única para la elección del número óptimo de clústeres. Al contrario, hay muchos métodos para estimar el mejor número de clústeres y, obviamente, no todos ellos dan el mismo resultado. Se recomienda considerar los resultados de diferentes métodos y explorar varios números de clústeres buscando siempre una coherente interpretación de los resultados.</p>
<p>En el ejemplo, eligiendo <span class="math inline">\(3\)</span> (quizás no sea la elección más evidente, pero ¿será interpretable?) como el número de clusters, podemos obtener los resultados finales tratando de nombrar los clusters:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>df_scaled2<span class="ot">=</span><span class="fu">as.data.frame</span>(df_scaled2)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>df_scaled2 <span class="sc">%&gt;%</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Cluster =</span> k3<span class="sc">$</span>cluster) <span class="sc">%&gt;%</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Cluster) <span class="sc">%&gt;%</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise_all</span>(<span class="st">"mean"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 3 × 9
  Cluster winner_rank loser_rank winner_age loser_age winner_rank_points
    &lt;int&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;              &lt;dbl&gt;
1       1     -0.708      -0.377    -0.561    -0.0684              2.36 
2       2      0.0519     -0.116     0.178     0.139              -0.211
3       3      0.0153     -0.109     0.0232   -0.190              -0.283
# ℹ 3 more variables: loser_rank_points &lt;dbl&gt;, w_1stWon &lt;dbl&gt;, l_1stWon &lt;dbl&gt;</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(parameters)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>res_kmeans <span class="ot">&lt;-</span> <span class="fu">cluster_analysis</span>(df_scaled2,</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">n =</span> <span class="dv">3</span>,</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">"kmeans"</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a> )</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">summary</span>(res_kmeans))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tarea
</div>
</div>
<div class="callout-body-container callout-body">
<p>Intenta “nombrar” los <span class="math inline">\(4\)</span> clusters del ejemplo. Para ello deberías fijarte también en las componentes principales.</p>
</div>
</div>
</section>
</section>
<section id="cluster-jerárquico" class="level2" data-number="5.8">
<h2 data-number="5.8" class="anchored" data-anchor-id="cluster-jerárquico"><span class="header-section-number">5.8</span> Cluster Jerárquico</h2>
<p>Las técnicas de agrupamiento jerárquico generan una clasificación iterativa de clústeres anidados mediante la unión o la separación de clústeres creados en etapas anteriores. Existen dos alternativas posibles:</p>
<ul>
<li><p><strong>Aglomerativos</strong>: en la versión aglomerativa, cada observación comienza siendo un clúster, y en cada iteración se unen en un único clúster los dos clústeres más similares, hasta alcanzar una situación final en la que todos las observaciones pertenecen a un único clúster. Esta versión se conoce como <strong>AGNES</strong> (“Agglomerative Nesting”).</p></li>
<li><p><strong>Divisivos</strong>: en la versión divisiva, todas las observaciones comienzan en un único clúster y las divisiones se realizan de forma recursiva, a medida que se desciende en la jerarquía, terminando cada observación formando un único clúster individual. Esta versión se conoce como <strong>DIANA</strong> (“Divise Analysis”).</p></li>
</ul>
<section id="cluster-jerárquico-aglomerativo" class="level3" data-number="5.8.1">
<h3 data-number="5.8.1" class="anchored" data-anchor-id="cluster-jerárquico-aglomerativo"><span class="header-section-number">5.8.1</span> Cluster jerárquico aglomerativo</h3>
<p>El clustering aglomerativo comienza con <span class="math inline">\(n\)</span> conglomerados (uno por cada dato) y, en cada paso, va fusionando los 2 grupos más similares hasta que hay un único grupo que contiene al total de datos.</p>
<p><strong>Algoritmo</strong></p>
<ol type="1">
<li><p>Input: matriz de disimilitud <span class="math inline">\(D=(d_{ij}),i,j=1,\dots, n\)</span></p>
<p>Inicializar los clusters: <span class="math inline">\(n\)</span> clusters <span class="math inline">\(S_i=\{i\}, i=1,\dots,n\)</span></p>
<p>Inicializar el conjunto de clusters que faltan por unir: <span class="math inline">\(Q=\{1,\dots,n\}\)</span></p></li>
<li><p>Seleccionar los 2 clusters más similares <span class="math inline">\(S_j\)</span> y <span class="math inline">\(S_k\)</span> : <span class="math inline">\(argmin_{j,k \in Q} \ d_{jk}\)</span></p>
<p>Con ellos, crear un nuevo cluster: <span class="math inline">\(S_l \leftarrow \{S_j \cup S_k \}\)</span></p>
<p>Guardar dichos clusters como no disponibles: <span class="math inline">\(Q \leftarrow Q\backslash \{j,k\}\)</span></p>
<p>Si <span class="math inline">\(S_l \neq \{1,\dots,n\}\)</span> entonces: <span class="math inline">\(Q \leftarrow Q\cup \{l\}\)</span></p>
<p>Para cada <span class="math inline">\(i \in Q\)</span>, actualizar la matriz de disimilitud <span class="math inline">\(d(i,l)\)</span></p></li>
<li><p>Repetir el paso 2 hasta que no queden cluster por unir</p></li>
</ol>
<p>Se necesita un criterio de conexión o “<em>linkage</em>” que especifique cómo se determina el parecido (o la disimilitud) entre dos clústeres. Este criterio no es único. Algunos de los criterios más comunes son:</p>
<div class="callout callout-style-default callout-note callout-titled" title="Método de Ward">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Método de Ward
</div>
</div>
<div class="callout-body-container callout-body">
<p>Minimiza la suma de las diferencias cuadradas dentro de los clústeres. Minimiza la varianza total dentro del conglomerado.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Agrupamiento de enlace completo">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Agrupamiento de enlace completo
</div>
</div>
<div class="callout-body-container callout-body">
<p>Minimiza la disimilitud máxima entre las observaciones de dos clústeres. Calcula todas las disimilitudes por pares entre los elementos del conglomerado A y los elementos del conglomerado B, y considera el mayor valor (es decir, el valor máximo) de estas disimilitudes como la distancia entre los dos conglomerados</p>
<p><span class="math display">\[
d_{EC}(A,B) = \max_{i \in A, i' \in B} d_{i,i'}
\]</span></p>
<p>Tiende a producir clusters más compactos.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Agrupamiento de enlace promedio">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Agrupamiento de enlace promedio
</div>
</div>
<div class="callout-body-container callout-body">
<p>Minimiza el promedio de las disimilitudes entre las observaciones de dos clústeres. Calcula todas las disimilitudes por pares entre los elementos del conglomerado A y los elementos del conglomerado B, y considera la media de estas disimilitudes como la distancia entre los dos conglomerados:</p>
<p><span class="math display">\[
d_{EP}(A,B) = \frac{1}{n_A n_B} \sum_{i \in A} \sum_{i' \in B} d_{i,i'}
\]</span></p>
<p>siendo <span class="math inline">\(n_A\)</span> y <span class="math inline">\(n_B\)</span> el número de elementos en los grupos <span class="math inline">\(A\)</span> y <span class="math inline">\(B\)</span>, respectivamente.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Agrupamiento de enlace mínimo o simple">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Agrupamiento de enlace mínimo o simple
</div>
</div>
<div class="callout-body-container callout-body">
<p>Minimiza las disimilitudes entre las observaciones más cercanas de dos clústeres. Es decir, calcula todas las disimilitudes por pares entre los elementos del conglomerado A y los elementos del conglomerado B, y considera la menor de estas disimilitudes como criterio de vinculación</p>
<p><span class="math display">\[
d_{ES}(A,B) = \min_{i \in A, i' \in B} d_{i,i'}
\]</span></p>
<p>Tiende a producir clusters largos y “dispersos”.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Agrupamiento de enlace de centroides">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Agrupamiento de enlace de centroides
</div>
</div>
<div class="callout-body-container callout-body">
<p>Calcula la disimilitud entre el centroide del conglomerado A y el centroide del conglomerado B. Agrupación de enlace de centroides.</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled" title="Para recordar">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Para recordar
</div>
</div>
<div class="callout-body-container callout-body">
<p>El criterio de agrupación o conexión es un parámetro fundamental en el resultado final del clustering jerárquico.</p>
</div>
</div>
</section>
<section id="cluster-jerárquico-divisivo" class="level3" data-number="5.8.2">
<h3 data-number="5.8.2" class="anchored" data-anchor-id="cluster-jerárquico-divisivo"><span class="header-section-number">5.8.2</span> Cluster jerárquico divisivo</h3>
<p>El clustering jerárquico divisivo es una estrategia de agrupamiento que sigue un enfoque descendente: comienza con todos los datos en un solo conglomerado y, a través de divisiones sucesivas, va creando subgrupos más pequeños hasta alcanzar un criterio de parada predefinido. Una de las técnicas más conocidas es el algorimo DIANA:</p>
<ul>
<li><p>Comienza con todos los datos en un único cluster y, recursivamente, divide cada cluster en 2 cluster hijo.</p></li>
<li><p>En cada paso el grupo más grande se divide hasta que cada objeto es un único conglomerado (u otro criterio de parada).</p></li>
<li><p>El clúster más <em>grande</em> será aquel de mayor diámetro, es decir, el que tiene mayor desemejanza entre dos de sus elementos o aquel con mayor desemejanza media. <!--#  Este diámetro tb se usa como la altura a la hora de representarlo en el dendrograma  --> Es decir, dados los cluster <span class="math inline">\(S=\{S_1,\dots,S_r\}\)</span>, el cluster más grande <span class="math inline">\(S_j\)</span> es aquel que</p>
<p><span class="math display">\[
S_j = argmax_{S_j \in S} \ d_{i,i'}, \forall i,i' \in S_j
\]</span></p></li>
<li><p>La observación más lejana <span class="math inline">\(i^{*} \in S_j\)</span>, <span class="math inline">\(i^{*}=argmax_{i \in S_j} \ d_{i,i'}\)</span>, es la que inicia el nuevo cluster <span class="math inline">\(S_{r+1}=\{i^{*}\}\)</span>.</p></li>
<li><p>Se asignan a este nuevo cluster <span class="math inline">\(S_{r+1}\)</span> los puntos que sean más cercanos a él que al cluster del que provienen <span class="math inline">\(S_j\)</span>.</p></li>
</ul>
<p>Hay otras alternativas para realizar clustering jerárquico divisivo. Por ejemplo, bisecting <span class="math inline">\(k\)</span>-means que divide el cluster de mayor diámetro en 2 cluster hijos haciendo uso de las <span class="math inline">\(k\)</span>-medias o los <span class="math inline">\(k\)</span>-medoides.</p>
</section>
<section id="cluster-jerárquico-en-r" class="level3" data-number="5.8.3">
<h3 data-number="5.8.3" class="anchored" data-anchor-id="cluster-jerárquico-en-r"><span class="header-section-number">5.8.3</span> Cluster jerárquico en R</h3>
<p>Existen diferentes funciones disponibles en R para calcular el clustering jerárquico. Las funciones más utilizadas son:</p>
<ul>
<li><code>hclust</code> [en el paquete <code>stats</code>] y <code>agnes</code> [en el paquete <code>cluster</code>] para el clustering jerárquico aglomerativo</li>
<li><code>diana</code> [en el paquete <code>cluster</code>] para clustering jerárquico divisivo</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Clustering jerárquico usando enlace completo</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>hc2 <span class="ot">&lt;-</span> <span class="fu">agnes</span>(df_scaled2, <span class="at">method =</span> <span class="st">"complete"</span> )</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>hc2<span class="sc">$</span>ac</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.8908784</code></pre>
</div>
</div>
<p>El <strong>coeficiente aglomerativo</strong> mide la cantidad de estructura de agrupamiento encontrada (los valores más cercanos a <span class="math inline">\(1\)</span> sugieren una fuerte estructura de agrupamiento).</p>
<p>Esto nos permite encontrar ciertos métodos de clustering jerárquico que pueden identificar estructuras de agrupación más fuertes. Aquí vemos que el método de Ward identifica la estructura de agrupación más fuerte de los cuatro métodos evaluados.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Métodos evaluados</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="st">"average"</span>, <span class="st">"single"</span>, <span class="st">"complete"</span>, <span class="st">"ward"</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(m) <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="st">"average"</span>, <span class="st">"single"</span>, <span class="st">"complete"</span>, <span class="st">"ward"</span>)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Función para calcular el coeficiente de agrupamiento</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>ac <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">agnes</span>(df_scaled2, <span class="at">method =</span> x)<span class="sc">$</span>ac</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="fu">map_dbl</span>(m, ac)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  average    single  complete      ward 
0.8498240 0.7175544 0.8908784 0.9465550 </code></pre>
</div>
</div>
<p>En ocasiones se emplea una representación gráfica en forma de árbol llamada <strong>dendrograma</strong> que ilustra las agrupaciones derivadas de la aplicación de una técnica de agrupamiento jerárquico. En el eje de ordenadas se presenta la distancia a la que se unen los diferentes clústeres. Las observaciones aparecen en el eje de abscisas.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Matriz de disimilaridades</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">dist</span>(df_scaled2, <span class="at">method =</span> <span class="st">"euclidean"</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Clustering jerárquico usando enlace completo</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>hc1 <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d, <span class="at">method =</span> <span class="st">"complete"</span> )</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Dendrograma</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc1, <span class="at">cex =</span> <span class="fl">0.6</span>, <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Fíjate cómo obtenemos diferentes resultados según el método propuesto.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>hc2 <span class="ot">&lt;-</span> <span class="fu">agnes</span>(df_scaled2, <span class="at">method =</span> <span class="st">"ward"</span> )</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Drendrograma</span></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="fu">pltree</span>(hc2, <span class="at">cex =</span> <span class="fl">0.6</span>, <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">main =</span> <span class="st">"Dendrograma de AGNES"</span>) </span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>La pregunta a la que nos enfrentamos es a qué distancia cortar el dendrograma, es decir, dónde dibujar una línea horizontal que determine el número óptimo de clústeres. Por ejemplo, en nuestro caso, cortar en <span class="math inline">\(10\)</span> generaría dos clústeres. Sin embargo, cortar en <span class="math inline">\(5\)</span> generaría cuatro clústeres, dos a la izquierda, dos a la derecha.</p>
<p>A continuación aplicamos el método divisivo.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Clustering jerárquico divisivo</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>hc4 <span class="ot">&lt;-</span> <span class="fu">diana</span>(df_scaled2)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Coeficiente de división; cantidad de estructura de agrupación encontrada</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>hc4<span class="sc">$</span>dc</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.8767046</code></pre>
</div>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.8514345</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Drendrograma</span></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="fu">pltree</span>(hc4, <span class="at">cex =</span> <span class="fl">0.6</span>, <span class="at">hang =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">main =</span> <span class="st">"Dendrogram de DIANA"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>En el dendrograma anterior, cada hoja corresponde a una observación. A medida que ascendemos en el árbol, las observaciones que son similares entre sí se combinan en ramas, que a su vez se fusionan a mayor altura.</p>
<p>La altura de la fusión, que figura en el eje vertical, indica la (di)similitud entre dos observaciones. Cuanto mayor es la altura de la fusión, menos similares son las observaciones.</p>
<div class="callout callout-style-default callout-caution callout-titled" title="Atención">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Atención
</div>
</div>
<div class="callout-body-container callout-body">
<p>Cuando empleamos un <strong>dendrograma</strong>, las conclusiones sobre la proximidad de dos observaciones sólo pueden extraerse a partir de la altura a la que se fusionan las ramas que contienen primero esas dos observaciones. No podemos utilizar la proximidad de dos observaciones a lo largo del eje horizontal como criterio de su similitud.</p>
</div>
</div>
<p>Tal como hemos indicado, la altura del corte del dendrograma controla el número de clusters obtenidos. Desempeña el mismo papel que la <span class="math inline">\(k\)</span> en la agrupación <span class="math inline">\(k\)</span>-means. Para identificar subgrupos (es decir, clusters), podemos cortar el dendrograma con la función <code>cutree</code> de R:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Método de Ward</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>hc5 <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d, <span class="at">method =</span> <span class="st">"ward.D2"</span> )</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Cortamos en 4 clusters</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>sub_grp <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc5, <span class="at">k =</span> <span class="dv">4</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualizamos el corte en el dendrograma</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc5, <span class="at">cex =</span> <span class="fl">0.6</span>)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="fu">rect.hclust</span>(hc5, <span class="at">k =</span> <span class="dv">4</span>, <span class="at">border =</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Número de observaciones en cada cluster</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(sub_grp)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>sub_grp
  1   2   3   4 
 57 172  45  26 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualización</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(<span class="fu">list</span>(<span class="at">data=</span>df_scaled2,<span class="at">cluster=</span>sub_grp))</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-22-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Podemos ir un paso más allá y comparar dos dendrogramas. En este ejemplo comparamos los resultados obtenidos con el método de “Ward” frente al “completo”.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dendextend)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Matriz de distancias</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>res.dist <span class="ot">&lt;-</span> <span class="fu">dist</span>(df_scaled2, <span class="at">method =</span> <span class="st">"euclidean"</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calcuamos los dos clustering jerárquicos</span></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>hc1 <span class="ot">&lt;-</span> <span class="fu">hclust</span>(res.dist, <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>hc2 <span class="ot">&lt;-</span> <span class="fu">hclust</span>(res.dist, <span class="at">method =</span> <span class="st">"ward.D2"</span>)</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Dendrogramas</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>dend1 <span class="ot">&lt;-</span> <span class="fu">as.dendrogram</span> (hc1)</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>dend2 <span class="ot">&lt;-</span> <span class="fu">as.dendrogram</span> (hc2)</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="co"># los enfrentamos</span></span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a><span class="fu">tanglegram</span>(dend1, dend2)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>El resultado muestra nodos “únicos”, con una combinación de etiquetas/elementos no presentes en el otro árbol, resaltados con líneas discontinuas. La calidad de la alineación de los dos árboles puede medirse utilizando la función de entrelazamiento. El entrelazamiento es una medida entre <span class="math inline">\(1\)</span> (entrelazamiento total) y <span class="math inline">\(0\)</span> (sin entrelazamiento). Un coeficiente de entrelazamiento menor corresponde a una buena alineación.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>dend_list <span class="ot">&lt;-</span> <span class="fu">dendlist</span>(dend1, dend2)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="fu">tanglegram</span>(dend1, dend2,</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">highlight_distinct_edges =</span> <span class="cn">FALSE</span>, <span class="co"># Turn-off dashed lines</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">common_subtrees_color_lines =</span> <span class="cn">FALSE</span>, <span class="co"># Turn-off line colors</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">common_subtrees_color_branches =</span> <span class="cn">TRUE</span>, <span class="co"># Color common branches </span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">main =</span> <span class="fu">paste</span>(<span class="st">"entanglement ="</span>, <span class="fu">round</span>(<span class="fu">entanglement</span>(dend_list), <span class="dv">2</span>))</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Tal y como hacíamos en el clustering no jerárquico, podemos aplicar métodos para determinar el número óptimo de clusters. Por ejemplo, el método del codo:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(df, <span class="at">FUN =</span> hcut, <span class="at">method =</span> <span class="st">"wss"</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>En el ejemplo propuesto, elegir <span class="math inline">\(4\)</span> como número óptimo parece una buena elección. Sin embargo, y como pasaba en los métodos no jerárquicos, métodos alternativos pueden llevarnos a soluciones alternativas.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co">#fviz_nbclust(df, FUN = hcut, method = "silhouette")</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co">#gap_stat &lt;- clusGap(df, FUN = hcut, nstart = 25, K.max = 10, B = 50)</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="co">#fviz_gap_stat(gap_stat)</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Ventajas del clustering jerárquico">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ventajas del clustering jerárquico
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Jerarquía de clusters</strong>: El clustering jerárquico crea una jerarquía de clusters que permite analizar los datos a diferentes niveles de granularidad. Es posible, por tanto, explorar tanto clusters globales como subgrupos más específicos. Al tener una jerarquía de clusters, puedes tomar decisiones a diferentes niveles de detalle. Esto es valioso para la segmentación de mercado, la taxonomía de especies, la organización de documentos, entre otros.</p></li>
<li><p><strong>Interpretación visual</strong>: El dendrograma facilita la interpretación visual de cómo se agrupan los datos y cómo se relacionan entre sí.</p></li>
<li><p><strong>No requiere especificación previa del número de clusters</strong>: A diferencia de algunos algoritmos de clustering que requieren que especifiques el número de clusters de antemano como <span class="math inline">\(k\)</span>-medias, el clustering jerárquico no necesita esta información.</p></li>
<li><p><strong>Identificación de subgrupos</strong>: El clustering jerárquico es eficaz para la identificación de subgrupos dentro de clusters más grandes. Esto es útil en áreas como la segmentación de clientes, donde se pueden tener clusters generales y luego identificar subgrupos más específicos.</p></li>
<li><p><strong>Detección de outliers</strong>: El clustering jerárquico puede ayudar a identificar outliers (valores atípicos) que no se ajustan a ningún cluster específico y que pueden ser importantes en el análisis de datos.</p></li>
<li><p><strong>No sensible a la inicialización</strong>: A diferencia de algunos algoritmos de clustering, como el <span class="math inline">\(k\)</span>-means, el clustering jerárquico no es sensible a la inicialización de centroides, lo que puede ayudar a evitar soluciones subóptimas.</p></li>
<li><p><strong>Análisis exploratorio de datos</strong>: El clustering jerárquico es útil en la exploración inicial de datos, ya que proporciona una visión general de cómo se agrupan naturalmente los datos sin la necesidad de conocimiento previo.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Desventajas del clustering jerárquico">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Desventajas del clustering jerárquico
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Requiere definir un criterio de corte</strong>: Para convertir la jerarquía en clusters finales, es necesario definir un criterio de corte en el dendrograma. Esta elección puede ser subjetiva y afectar los resultados. Así mismo hay que definir el tipo de enlace a emplear (y la medida de disimilitud) Cada una de estas decisiones puede influir mucho en los resultados obtenidos. En la práctica, probamos varias opciones diferentes y buscamos la que ofrece la solución más útil o interpretable.</p></li>
<li><p>Con estos métodos, <strong>no hay una única respuesta correcta</strong>: debe considerarse cualquier solución que exponga algunos aspectos interesantes de los datos.</p></li>
<li><p><strong>No es óptimo para todos los tipos de datos</strong>: El clustering jerárquico funciona mejor cuando los clusters tienen una estructura jerárquica natural. En algunos casos, donde no existe una jerarquía clara, otros métodos de clustering pueden ser más apropiados.</p></li>
<li><p><strong>No es adecuado para datos de alta dimensión</strong>: El rendimiento del clustering jerárquico puede disminuir en conjuntos de datos de alta dimensión debido a la maldición de la dimensionalidad. Este fenómeno significa que al aumentar el número de dimensiones de un problema se pueden agravar muchos de los problemas que aparecen en dimensiones bajas (curse of dimensionality, <span class="citation" data-cites="bellman1961reduction">Bellman y Kalaba (<a href="references.html#ref-bellman1961reduction" role="doc-biblioref">1961</a>)</span>).</p></li>
<li><p><strong>No siempre produce resultados reproducibles</strong>: La estructura jerárquica resultante puede variar según la métrica de distancia y el enfoque de enlace utilizado, lo que puede dar lugar a resultados no siempre reproducibles.</p></li>
<li><p><strong>Sin capacidad de predicción</strong>: Las técnicas de agrupamiento jerárquicas no son útiles a la hora de predecir el clúster al que pertenecen nuevas observaciones.</p></li>
</ul>
</div>
</div>
</div>
</section>
</section>
<section id="mapas-auto-organizados" class="level2" data-number="5.9">
<h2 data-number="5.9" class="anchored" data-anchor-id="mapas-auto-organizados"><span class="header-section-number">5.9</span> Mapas auto-organizados</h2>
<p>Los Mapas Auto-organizados de Kohonen (SOM por sus siglas en inglés, “<em>Self-Organizing Maps</em>”) no solo son una poderosa herramienta para la <strong>reducción de la dimensión</strong>, sino que también son ampliamente utilizados como algoritmo de clustering. Aunque la reducción de la dimensión es una de sus aplicaciones más destacadas, los SOM también tienen la capacidad de agrupar datos de manera efectiva.</p>
<p>El proceso de clustering con SOM implica organizar datos en grupos o clusters de manera que los elementos dentro de un mismo grupo sean similares entre sí en función de ciertas características. Un SOM consiste en una malla de neuronas organizadas en una estructura topológica bidimensional, usualmente una cuadrícula rectangular o hexagonal. Cada neurona tiene un vector de pesos asociado, de la misma dimensión que los datos de entrada.</p>
<p>Sea un conjunto de datos de entrada <span class="math inline">\(\mathbf{X} \in \mathbb{R}^p\)</span>, el SOM está compuesto por una malla bidimensional de <span class="math inline">\(K\)</span> neuronas o prototipos. Cada neurona está “definida” por un vector de pesos <span class="math inline">\(\mathbf{w}_j \in \mathbb{R}^p\)</span>, donde <span class="math inline">\(j\)</span> es el índice de la neurona en la malla. Cada neurona representa una ubicación en el espacio SOM.</p>
<p>A continuación se explica cómo funciona el algoritmo:</p>
<ol type="1">
<li><p><strong>Inicialización</strong>: Se inicializan los pesos <span class="math inline">\(\mathbf{w}_j\)</span> de las neuronas aleatoriamente (¿Qué técnica, ya estudiada en la asignatura, se te ocurre para inicializar los pesos de las neuronas?). <!--# PCA --></p></li>
<li><p><strong>Selección del vector de entrada</strong>: En cada iteración, se selecciona aleatoriamente un vector de entrada <span class="math inline">\(\mathbf{x}_i\)</span> del conjunto de datos.</p></li>
<li><p><strong>Cálculo de la neurona ganadora</strong>: Se encuentra la neurona cuyo vector de pesos <span class="math inline">\(\mathbf{w}_j\)</span> sea más cercano a <span class="math inline">\(\mathbf{x}_i\)</span> en términos de distancia Euclídea: <span class="math display">\[j^* = \arg\min_{j} \| \mathbf{x}_i - \mathbf{w}_j \|.\]</span> Es decir, encontramos su prototipo o neurona más cercana. Esta neurona es conocida como BMU (Best Matching Unit).</p></li>
<li><p><strong>Actualización de pesos</strong>: Se actualizan los pesos de la BMU y de sus vecinos en base a la siguiente ecuación: <span class="math display">\[    \mathbf{w}_j(t+1) = \mathbf{w}_j(t) + \alpha(t) h_{j,j^*}(t) (\mathbf{x}_i - \mathbf{w}_j(t)),\]</span> donde:</p>
<ul>
<li><p><span class="math inline">\(\alpha(t)\)</span> es la tasa de aprendizaje, que decrece con el tiempo <span class="math inline">\(t\)</span> (número de iteraciones)</p></li>
<li><p><span class="math inline">\(h_{j,j^*}(t)\)</span> es la función de vecindad que decrece con la distancia entre <span class="math inline">\(j\)</span> e <span class="math inline">\(j^{*}\)</span> en la malla, comúnmente definida como una función gaussiana: <span class="math display">\[h_{j,j^*}(t) = \exp\left(-\frac{\| r_j - r_{j^*} \|^2}{2 \sigma^2(t)}\right),\]</span> donde <span class="math inline">\(r_j\)</span> es la posición de la neurona <span class="math inline">\(j\)</span> en la malla y <span class="math inline">\(\sigma(t)\)</span> es el radio de vecindad, que también decrece con el tiempo.</p></li>
</ul>
<p>Esta actualización se hace tanto para la neurona ganadora como para sus vecinas.</p></li>
</ol>
<p>Es decir, se presentan los datos al SOM, y cada dato se asigna a la neurona cuyos pesos son más similares a los atributos del dato. Las neuronas ganadoras (aquellas a las que se asigna un dato) y sus vecinas en el mapa SOM se ajustan para que se parezcan más al dato presentado.</p>
<ol start="5" type="1">
<li><strong>Repetición</strong>: Se repiten los pasos anteriores por un número determinado de iteraciones o hasta la convergencia del modelo.</li>
</ol>
<p>Después del entrenamiento, las neuronas en el mapa SOM que están cerca una de la otra representan clusters de datos. Los datos que se asignaron a estas neuronas durante el entrenamiento se consideran miembros de un mismo cluster. Los SOM tienen propiedades interesantes como la preservación topológica, garantizando que las neuronas cercanas en la malla tendrán pesos similares.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Función objetivo">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Función objetivo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>El algoritmo de entrenamiento de los Mapas de Kohonen (SOM) puede entenderse como un proceso de minimización de una función de coste mediante descenso de gradiente.</p>
<p>El objetivo del entrenamiento en los SOM es minimizar la siguiente función de coste: <span class="math display">\[    E = \sum_{x_i \in \mathbf{X}} \sum_{j} h_{j,j^*} \| \mathbf{x}_i - \mathbf{w}_j \|^2\]</span> donde:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{X}\)</span> es el conjunto de datos de entrada.</p></li>
<li><p><span class="math inline">\(j^*\)</span> es la neurona ganadora para cada <span class="math inline">\(\mathbf{x}_i\)</span>.</p></li>
<li><p><span class="math inline">\(h_{j,j^*}\)</span> es la función de vecindad que determina la influencia de la neurona <span class="math inline">\(j\)</span> en la actualización de pesos.</p></li>
</ul>
<p>Esta función de coste representa una combinación de dos principios: 1. La asignación de cada <span class="math inline">\(\mathbf{x}_i\)</span> a una neurona con pesos <span class="math inline">\(\mathbf{w}_j\)</span> minimiza la distancia entre los datos y los prototipos. 2. Preservación topológica: La regularización impuesta por <span class="math inline">\(h_{j,j^*}\)</span> garantiza que neuronas cercanas tengan pesos similares, permitiendo que el mapa refleje la estructura del espacio de entrada.</p>
<p>Veámos cómo se llega a la actualización de las neuronas. Como buscamos minimizar <span class="math inline">\(E\)</span>, aplicaremos el descenso del gradiente sobre los pesos que definen cada neurona <span class="math inline">\(\mathbf{w}_j\)</span>. Esto es, derivamos la función de coste con respecto a <span class="math inline">\(\mathbf{w}_j\)</span>: <span class="math display">\[\frac{\partial E}{\partial \mathbf{w}_j} = -\sum_{\mathbf{x}_i \in \mathbf{X}} h_{j,j^*} 2 (\mathbf{x}_i - \mathbf{w}_j).\]</span></p>
<p>El ajuste de pesos se obtiene usando la regla de actualización:</p>
<p><span class="math display">\[\mathbf{w}_j(t+1) = \mathbf{w}_j(t) - \alpha(t) \frac{\partial E}{\partial \mathbf{w}_j}.\]</span></p>
<p>Sustituyendo <span class="math inline">\(\frac{\partial E}{\partial \mathbf{w}_i}\)</span>, obtenemos:</p>
<p><span class="math display">\[\mathbf{w}_j(t+1) = \mathbf{w}_j(t) + \alpha(t) \sum_{\mathbf{x}_i \in \mathbf{X}} h_{j,j^*}(t) (\mathbf{x}_i - \mathbf{w}_j(t)).\]</span></p>
<p>Por motivos de coste computacional, este ajuste se realiza para la observación particular que ha sido escogida en la iteración, es decir, para <span class="math inline">\(\mathbf{x}_i\)</span> en lugar de para todo los puntos del conjunto de datos. Así: <span class="math display">\[\mathbf{w}_j(t+1) = \mathbf{w}_j(t) + \alpha(t) h_{j,j^*}(t) (\mathbf{x}_i - \mathbf{w}_j(t)),\]</span></p>
<p>donde:</p>
<ul>
<li><p><span class="math inline">\(\alpha(t)\)</span> es la tasa de aprendizaje que decrece con el tiempo (iteraciones).</p></li>
<li><p><span class="math inline">\(h_{j,j^*}(t)\)</span> es la función de vecindad que decrece con la distancia en la malla y con el tiempo. La idea es que en las primeras iteraciones, la vecindad es amplia, permitiendo un ajuste global del mapa SOM. En las últimas iteraciones, la vecindad es pequeña y los pesos de las neuronas se ajustan más localmente.</p></li>
</ul>
<p>Aquí vemos que la actualización de los pesos se mueve en dirección a <span class="math inline">\(\mathbf{x}_i\)</span>, ponderada por la tasa de aprendizaje <span class="math inline">\(\alpha(t)\)</span> y la función de vecindad <span class="math inline">\(h_{j,j^*}(t)\)</span>​, lo que permite que las neuronas cercanas a la BMU también se ajusten gradualmente.</p>
<!--# Relación con el algoritmo K-means: Cuando el radio de vecindad $\sigma(t)$ se reduce significativamente, el modelo se comporta de manera similar a K-means con actualización en línea. Sin embargo, a diferencia de K-means, el SOM preserva la estructura topológica de los datos al imponer restricciones espaciales en la actualización de los pesos.  -->
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Ventajas">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-26-contents" aria-controls="callout-26" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-26" class="callout-26-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Topología preservada</strong>: Una ventaja clave de los SOM en el clustering es que preservan la topología de los datos. Esto significa que los clusters en el SOM reflejan la estructura de vecindad en los datos originales, lo que facilita la interpretación de los resultados.</p></li>
<li><p><strong>Escalabilidad</strong>: Los SOM pueden manejar grandes conjuntos de datos y dimensiones elevadas, lo que los hace útiles para aplicaciones del mundo real con datos complejos.</p></li>
<li><p><strong>Flexibilidad</strong>: Los SOM pueden utilizarse con diversos algoritmos de agrupamiento, lo que permite adaptarlos a diferentes tipos de datos y objetivos de análisis.</p></li>
<li><p><strong>Visualización</strong>: La representación en un espacio bidimensional o tridimensional facilita la visualización de datos complejos, lo que permite una comprensión más intuitiva.</p></li>
<li><p><strong>Exploración interactiva:</strong> Los SOM permiten la exploración interactiva de datos, ya que los usuarios pueden navegar por el mapa para inspeccionar las regiones y sus contenidos.</p></li>
<li><p><strong>Reducción de ruido:</strong> Los SOM a menudo ayudan a reducir el ruido y la redundancia en los datos, lo que mejora la calidad del análisis.</p></li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Desventajas">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-27-contents" aria-controls="callout-27" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Desventajas
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-27" class="callout-27-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p><strong>Sensibilidad a la inicialización</strong>: Los SOM son sensibles a la inicialización de los pesos de las neuronas. Los resultados pueden variar significativamente según cómo se configuren los pesos iniciales, lo que significa que pueden converger a soluciones subóptimas si no se seleccionan adecuadamente los valores iniciales.</p></li>
<li><p><strong>Determinación del tamaño del mapa</strong>: Elegir el tamaño adecuado para el mapa SOM puede ser un desafío. Si el mapa es demasiado pequeño, puede no capturar la estructura de los datos correctamente, mientras que si es demasiado grande, puede sobreajustarse a los datos y perder la capacidad de generalización.</p></li>
<li><p><strong>Interpretación de los resultados</strong>: La interpretación de los resultados de un SOM puede ser complicada, especialmente en mapas de alta dimensión. Mapear los clusters y las relaciones entre las neuronas en el mapa a menudo requiere conocimiento experto del dominio.</p></li>
<li><p><strong>Requiere ajuste de hiperparámetros</strong>: El rendimiento de un SOM puede depender de la elección adecuada de hiperparámetros, como la tasa de aprendizaje, la vecindad de las neuronas y el número de iteraciones. En algunos casos, encontrar los valores óptimos puede ser un proceso de prueba y error.</p></li>
<li><p><strong>Puede converger a mínimos locales</strong>: Como con muchos algoritmos de optimización, los SOM pueden converger a mínimos locales en lugar del mínimo global, lo que puede llevar a soluciones subóptimas.</p></li>
<li><p><strong>Requiere grandes conjuntos de datos</strong>: Los SOM pueden no funcionar bien en conjuntos de datos pequeños o altamente desequilibrados, ya que su eficacia se basa en la capacidad de aprender patrones significativos a partir de una cantidad suficiente de datos.</p></li>
</ul>
</div>
</div>
</div>
<p>Vamos a estudiar su implementación en R con la libraría <code>kohonen</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kohonen)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'kohonen'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following object is masked from 'package:purrr':

    map</code></pre>
</div>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caTools)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>bank <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">'https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv'</span>)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2938</span>)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>sample<span class="ot">&lt;-</span><span class="fu">sample.split</span>(bank<span class="sc">$</span>deposit,<span class="at">SplitRatio=</span><span class="fl">0.5</span>)</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>bank.train <span class="ot">&lt;-</span> <span class="fu">subset</span>(bank,sample<span class="sc">==</span><span class="cn">TRUE</span>)</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>df<span class="ot">=</span> bank.train <span class="sc">%&gt;%</span> </span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a>      <span class="fu">filter</span>(balance<span class="sc">&gt;</span><span class="dv">0</span> <span class="sc">&amp;</span> previous<span class="sc">&gt;</span><span class="dv">0</span> <span class="sc">&amp;</span> pdays<span class="sc">&gt;</span><span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>( <span class="at">log.balance=</span><span class="fu">log</span>(balance),</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a>              <span class="at">log.age=</span><span class="fu">log</span>(age),</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a>              <span class="at">log.campaign=</span><span class="fu">log</span>(campaign),</span>
<span id="cb58-13"><a href="#cb58-13" aria-hidden="true" tabindex="-1"></a>              <span class="at">log.previous=</span><span class="fu">log</span>(previous),</span>
<span id="cb58-14"><a href="#cb58-14" aria-hidden="true" tabindex="-1"></a>              <span class="at">log.pdays =</span> <span class="fu">log</span>(pdays),</span>
<span id="cb58-15"><a href="#cb58-15" aria-hidden="true" tabindex="-1"></a>              <span class="at">log.duration=</span><span class="fu">log</span>(duration)) <span class="sc">%&gt;%</span></span>
<span id="cb58-16"><a href="#cb58-16" aria-hidden="true" tabindex="-1"></a>      <span class="fu">select</span>(log.balance,log.age,log.campaign,log.duration,log.previous,log.pdays,deposit)</span>
<span id="cb58-17"><a href="#cb58-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-18"><a href="#cb58-18" aria-hidden="true" tabindex="-1"></a>df.st <span class="ot">&lt;-</span> <span class="fu">scale</span>(df[,<span class="sc">-</span><span class="dv">7</span>])</span>
<span id="cb58-19"><a href="#cb58-19" aria-hidden="true" tabindex="-1"></a>df.som <span class="ot">&lt;-</span> kohonen<span class="sc">::</span><span class="fu">som</span>(<span class="fu">as.matrix</span>( df.st),<span class="fu">somgrid</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="st">"hexagonal"</span>))</span>
<span id="cb58-20"><a href="#cb58-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-21"><a href="#cb58-21" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(df.som)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "data"             "unit.classif"     "distances"        "grid"            
 [5] "codes"            "changes"          "alpha"            "radius"          
 [9] "na.rows"          "user.weights"     "distance.weights" "whatmap"         
[13] "maxNA.fraction"   "dist.fcts"       </code></pre>
</div>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(df.som)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>SOM of size 4x4 with a hexagonal topology and a bubble neighbourhood function.
The number of data layers is 1.
Distance measure(s) used: sumofsquares.
Training data included: 1284 objects.
Mean distance to the closest unit in the map: 2.413.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># clustering</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df.som<span class="sc">$</span>unit.classif)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 13 10 10 10 15 15</code></pre>
</div>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># nombrar los clusters</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>df.som<span class="sc">$</span>codes</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
    log.balance     log.age log.campaign log.duration log.previous   log.pdays
V1   -0.1665504 -0.17250915   1.65905689  -2.89127875   0.75321963  0.76228798
V2   -0.5729971 -0.61916298   0.95649105   0.11360709  -0.61633860  0.53542329
V3   -2.7626873 -0.09630617  -0.22481605  -0.01775780  -0.43449514  0.39037929
V4   -0.2987460 -0.27867111  -0.48069577  -1.32194232  -0.32410665  0.60980323
V5    0.1692260 -0.29513795   1.09413528   0.06622639   1.81798163 -0.19898689
V6    0.2130609  0.79952586   1.92607192  -0.09957394   0.50713651  0.50409529
V7   -0.9925584 -0.25999586   0.06297161   0.80969889   1.13448930  0.29828886
V8   -0.5467391 -0.89454638  -0.75231500   0.05691503  -0.58716283 -0.44382395
V9    0.8004058  1.45723263  -0.46626151   0.25742877   1.15260783 -0.18585464
V10   0.8681725 -0.08718076   0.67816551   1.23176536  -0.35230246  0.09409626
V11   0.6203543 -0.43508207  -0.81201086  -0.24560976   0.77187053 -0.66260374
V12  -0.2636134 -0.53803480  -0.06677845  -0.28219715  -0.05178134 -4.61539128
V13   0.2996683  1.38922194  -0.83068969   0.29947979  -0.50925213 -0.25212519
V14   0.1516991  1.16041553   0.66077136  -0.28837446  -0.60162534  0.17447204
V15   0.4713511 -0.37448390  -0.83870156   0.73106396  -0.63544758  0.79393509
V16   0.6820996 -0.72646668   0.55768237  -0.42934114  -0.49679657 -0.42626768</code></pre>
</div>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(df.som, <span class="at">type=</span><span class="st">"codes"</span>)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(df.som, <span class="at">type=</span><span class="st">"mapping"</span>, <span class="at">col =</span> <span class="fu">as.numeric</span>(<span class="fu">as.factor</span>(df<span class="sc">$</span>deposit))<span class="sc">+</span><span class="dv">1</span>, <span class="at">pch=</span><span class="dv">20</span>)</span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="aprnosup_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># versión supervisada</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="co"># kohmap &lt;- xyf(as.matrix(df.st), as.factor(df$deposit),grid = somgrid(4, 4, "hexagonal"), rlen=100)</span></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="co">#plot(kohmap, type="codes", main = c("Codes X", "Codes Y"))</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="co">#plot(kohmap, type="mapping",  col = as.numeric(as.factor(df$deposit))+1, pch=20)</span></span></code><button title="Copiar al portapapeles" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bellman1961reduction" class="csl-entry" role="listitem">
Bellman, Richard, y Robert Kalaba. 1961. <span>«Reduction of dimensionality, dynamic programming, and control processes»</span>.
</div>
<div id="ref-goodall1966new" class="csl-entry" role="listitem">
Goodall, David W. 1966. <span>«A new similarity index based on probability»</span>. <em>Biometrics</em>, 882-907.
</div>
<div id="ref-gower1971general" class="csl-entry" role="listitem">
Gower, John C. 1971. <span>«A general coefficient of similarity and some of its properties»</span>. <em>Biometrics</em>, 857-71.
</div>
<div id="ref-hartigan1979algorithm" class="csl-entry" role="listitem">
Hartigan, John A, y Manchek A Wong. 1979. <span>«Algorithm AS 136: A k-means clustering algorithm»</span>. <em>Journal of the royal statistical society. series c (applied statistics)</em> 28 (1): 100-108.
</div>
<div id="ref-kelleher2020fundamentals" class="csl-entry" role="listitem">
Kelleher, John D, Brian Mac Namee, y Aoife D’arcy. 2020. <em>Fundamentals of machine learning for predictive data analytics: algorithms, worked examples, and case studies</em>. MIT press.
</div>
<div id="ref-xu2015comprehensive" class="csl-entry" role="listitem">
Xu, Dongkuan, y Yingjie Tian. 2015. <span>«A comprehensive survey of clustering algorithms»</span>. <em>Annals of Data Science</em> 2: 165-93.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./reddim.html" class="pagination-link" aria-label="Técnicas de reducción de la dimensionalidad">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Técnicas de reducción de la dimensionalidad</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./eval.html" class="pagination-link" aria-label="Medidas de rendimiento">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Medidas de rendimiento</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>